{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10843558-3e75-4b0a-807c-166b1370b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tinkoff.invest import Client, SecurityTradingStatus, CandleInterval\n",
    "from tinkoff.invest.services import InstrumentsService\n",
    "from tinkoff.invest.utils import quotation_to_decimal, now\n",
    "\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "\n",
    "import pickle\n",
    "def dump_pkl(data, filename):\n",
    "  with open(filename, 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "def load_pkl(filename):\n",
    "  with open(filename, 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "  return data\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d393b94-5d31-4aa1-b85b-0cb2da211f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41408bd-162d-4f94-896e-eabcb234ead4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f60bc2fd-d8fc-44c0-914e-60f8d4dd50a5",
   "metadata": {},
   "source": [
    "# 1. Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1430f0a3-bbe8-4d35-b6d7-7226331d5568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GetAccountsResponse(accounts=[Account(id='2169433725', type=<AccountType.ACCOUNT_TYPE_TINKOFF: 1>, name='M13', status=<AccountStatus.ACCOUNT_STATUS_OPEN: 2>, opened_date=datetime.datetime(2022, 4, 27, 0, 0, tzinfo=datetime.timezone.utc), closed_date=datetime.datetime(1970, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), access_level=<AccessLevel.ACCOUNT_ACCESS_LEVEL_READ_ONLY: 2>)])\n"
     ]
    }
   ],
   "source": [
    "TOKEN = 'xxx'\n",
    "with Client(TOKEN) as client:\n",
    "    print(client.users.get_accounts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5eba080-f5ae-43a2-a632-69578a1bf7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOT_TOKEN = '7598372117:AAHKSc-jHo02zSzJ-NOoZ8GloOqdbNySeZw'\n",
    "BOT_CHAT_ID = '131510115'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2393dbe4-6fe6-4c5d-9b10-b71502085b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES1 = ['close_1day_w5_norm_std/close_1day_w100_norm_std',\n",
    " 'tmos_close_1day_w5_ma_up_3std/tmos_close_1day',\n",
    " 'tmos_close_1day_w5_std/tmos_close_1day_w100_std',\n",
    " 'close_1day_w3_std/close_1day_w100_std',\n",
    " 'close_1day_w20_roc',\n",
    " 'close_w70_lvl_1.02-1.03',\n",
    " 'tmos_close_1day_w5_norm_std',\n",
    " 'volume_1day_w20_rsi',\n",
    " 'close_1day_w20_norm_std',\n",
    " 'close_w14_ma_low_3std/close',\n",
    " 'tmos_close_w14_max/tmos_close',\n",
    " 'close_1day_w5_ma/close_1day_w100_ma',\n",
    " 'tmos_close/tmos_close_w70_ma',\n",
    " 'tmos_close_w70_lvl_1-1.005',\n",
    " 'close_w14_mean_abs_pct/close_w70_mean_abs_pct',\n",
    " 'tmos_close_1day_w20_rsi',\n",
    " 'tmos_close_1day_w5_min/tmos_close_1day_w5_max',\n",
    " 'close_1day_w100_lvl_1.02-1.03',\n",
    " 'close_1day_w20_min/close_1day_w20_max',\n",
    " 'close_w14_mean_abs_pct']\n",
    "\n",
    "\n",
    "MODEL_FEATURES2 = ['tmos_close_w14_rsi',\n",
    " 'tmos_close_w70_lvl_1.01-1.015',\n",
    " 'tmos_close_w70_lvl_-0.985-0.99',\n",
    " 'tmos_close_1day_w20_lvl_1.005-1.01',\n",
    " 'tmos_close_1day_w100_lvl_1.04-1.05',\n",
    " 'tmos_close_w70_rsi',\n",
    " 'tmos_close_w70_roc',\n",
    " 'close_w70_rsi',\n",
    " 'hour',\n",
    " 'tmos_close_w70_ma_up_2std/tmos_close',\n",
    " 'tmos_close_w14_ma/tmos_close_w70_ma',\n",
    " 'close_w70_lvl_-0.98-0.99',\n",
    " 'tmos_close_w5_ma/tmos_close_w70_ma',\n",
    " 'tmos_close_w14_max/tmos_close',\n",
    " 'tmos_close_1day_w100_lvl_1.015-1.02',\n",
    " 'tmos_close_1day_w20_lvl_1.015-1.02',\n",
    " 'tmos_close_w5_max/tmos_close',\n",
    " 'close_w1_roc',\n",
    " 'tmos_close_1day_w3_mean_abs_pct/tmos_close_1day_w5_mean_abs_pct',\n",
    " 'tmos_close_1day_w5_ma_low_2std/tmos_close_1day']\n",
    "\n",
    "MODEL_FEATURES3 = []\n",
    "\n",
    "FEATURES = list(set(MODEL_FEATURES1 + MODEL_FEATURES2 + MODEL_FEATURES3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "061da235-0767-4c15-a6ef-e26b6148b632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f40e85e-24ba-4eba-9cc8-d49996066b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbffffcb-a7bf-4795-82ad-00e0a4cda519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COUNT_DAY_POINTS = 100\n",
    "COUNT_HOUR_POINTS = 70\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "STOCKS = ['AFLT',\n",
    " 'IRAO',\n",
    " 'MVID',\n",
    " 'OGKB',\n",
    " 'SPBE',\n",
    " 'MTLRP',\n",
    " 'SFIN',\n",
    " 'KMAZ',\n",
    " 'CBOM',\n",
    " 'MSNG',\n",
    " 'FEES',\n",
    " 'LKOH',\n",
    " 'FESH',\n",
    " 'KROT',\n",
    " 'LENT',\n",
    " 'MDMG',\n",
    " 'NKNCP',\n",
    " 'VKCO',\n",
    " 'UPRO',\n",
    " 'MRKY',\n",
    " 'SELG',\n",
    " 'SGZH',\n",
    " 'AKRN',\n",
    " 'CHMK',\n",
    " 'ENPG',\n",
    " 'YAKG',\n",
    " 'PMSBP',\n",
    " 'RKKE',\n",
    " 'GEMC',\n",
    " 'LSRG',\n",
    " 'MRKZ',\n",
    " 'BANE',\n",
    " 'CNTL',\n",
    " 'PMSB',\n",
    " 'KRKNP',\n",
    " 'NMTP',\n",
    " 'PLZL',\n",
    " 'TATN',\n",
    " 'AFKS',\n",
    " 'UNKL',\n",
    " 'APTK',\n",
    " 'LNZL',\n",
    " 'GMKN',\n",
    " 'NSVZ',\n",
    " 'LIFE',\n",
    " 'RNFT',\n",
    " 'VRSB',\n",
    " 'YDEX',\n",
    " 'UWGN',\n",
    " 'TGKN',\n",
    " 'ABRD',\n",
    " 'HYDR',\n",
    " 'ABIO',\n",
    " 'WUSH',\n",
    " 'RTKMP',\n",
    " 'GTRK',\n",
    " 'NLMK',\n",
    " 'TATNP',\n",
    " 'CNTLP',\n",
    " 'BLNG',\n",
    " 'ALRS',\n",
    " 'GCHE',\n",
    " 'VSMO',\n",
    " 'LSNG',\n",
    " 'TRNFP',\n",
    " 'MGNT',\n",
    " 'RENI',\n",
    " 'NKHP',\n",
    " 'TGKB',\n",
    " 'ROLO',\n",
    " 'IRKT',\n",
    " 'MGTSP',\n",
    " 'MTLR',\n",
    " 'TGKA',\n",
    " 'MSRS',\n",
    " 'HEAD',\n",
    " 'RUAL',\n",
    " 'MRKV',\n",
    " 'KLSB',\n",
    " 'SIBN',\n",
    " 'SNGS',\n",
    " 'MSTT',\n",
    " 'KAZTP',\n",
    " 'DVEC',\n",
    " 'NVTK',\n",
    " 'RASP',\n",
    " 'VTBR',\n",
    " 'PHOR',\n",
    " 'TTLK',\n",
    " 'NKNC',\n",
    " 'T',\n",
    " 'KAZT',\n",
    " 'AMEZ',\n",
    " 'MRKU',\n",
    " 'TRMK',\n",
    " 'MOEX',\n",
    " 'SBERP',\n",
    " 'X5',\n",
    " 'CHMF',\n",
    " 'RTKM',\n",
    " 'SMLT',\n",
    " 'LNZLP',\n",
    " 'MRKC',\n",
    " 'MRKP',\n",
    " 'UNAC',\n",
    " 'KZOSP',\n",
    " 'GAZP',\n",
    " 'ROSN',\n",
    " 'BELU',\n",
    " 'ELFV',\n",
    " 'FLOT',\n",
    " 'PIKK',\n",
    " 'LSNGP',\n",
    " 'SVAV',\n",
    " 'TGKBP',\n",
    " 'POSI',\n",
    " 'KZOS',\n",
    " 'MRKS',\n",
    " 'SNGSP',\n",
    " 'MTSS',\n",
    " 'MAGN',\n",
    " 'PRFN',\n",
    " 'SBER',\n",
    " 'BANEP',\n",
    " 'BSPB',\n",
    " 'AQUA',\n",
    " 'RBCM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e559c8-16f7-46b8-bd22-5e75c6711579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6767d8-501e-470b-a0cf-d2b5bb6274db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f87bd0-980d-4f69-90f9-5c136771b23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4cb446d-836d-4f47-9926-29168770ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPS_1HOUR = [\n",
    "        #1hour: w1, #w5, #w14, #w70\n",
    "         ['close_w1_roc', 'close_w5_alpha', 'close_w14_alpha', 'close_w70_alpha'],\n",
    "         ['volume_w1_roc', 'volume_w5_alpha', 'volume_w14_alpha', 'volume_w70_alpha'],\n",
    "         ['tmos_close_w1_roc', 'tmos_close_w5_alpha', 'tmos_close_w14_alpha', 'tmos_close_w70_alpha'],\n",
    "\n",
    "         ['close_w1_roc', 'close_w5_roc', 'close_w14_roc', 'close_w70_roc'],\n",
    "         ['volume_w1_roc', 'volume_w5_roc', 'volume_w14_roc', 'volume_w70_roc'],\n",
    "         ['tmos_close_w1_roc', 'tmos_close_w5_roc', 'tmos_close_w14_roc', 'tmos_close_w70_roc'],\n",
    "\n",
    "         ['close_w5_mean_abs_pct', 'close_w14_mean_abs_pct', 'close_w70_mean_abs_pct'],\n",
    "         ['volume_w5_mean_abs_pct', 'volume_w14_mean_abs_pct', 'volume_w70_mean_abs_pct'],\n",
    "         ['tmos_close_w5_mean_abs_pct', 'tmos_close_w14_mean_abs_pct', 'tmos_close_w70_mean_abs_pct'],\n",
    "\n",
    "         ['close_w5_std', 'close_w14_std', 'close_w70_std'],\n",
    "         ['volume_w5_std', 'volume_w14_std', 'volume_w70_std'],\n",
    "         ['tmos_close_w5_std', 'tmos_close_w14_std', 'tmos_close_w70_std'],\n",
    "\n",
    "         ['close_w5_norm_std', 'close_w14_norm_std', 'close_w70_norm_std'],\n",
    "         ['volume_w5_norm_std', 'volume_w14_norm_std', 'volume_w70_norm_std'],\n",
    "         ['tmos_close_w5_norm_std', 'tmos_close_w14_norm_std', 'tmos_close_w70_norm_std'],\n",
    "\n",
    "         ['close_w5_rsi', 'close_w14_rsi', 'close_w70_rsi'],\n",
    "         ['volume_w5_rsi', 'volume_w14_rsi', 'volume_w70_rsi'],\n",
    "         ['tmos_close_w5_rsi', 'tmos_close_w14_rsi', 'tmos_close_w70_rsi'],\n",
    "        \n",
    "         ['close', 'close_w5_ma', 'close_w14_ma', 'close_w70_ma'],\n",
    "         ['volume', 'volume_w5_ma', 'volume_w14_ma', 'volume_w70_ma'],\n",
    "         ['tmos_close', 'tmos_close_w5_ma', 'tmos_close_w14_ma', 'tmos_close_w70_ma'],\n",
    "\n",
    "         {'close' : ['close_w5_min', 'close_w14_min', 'close_w70_min']},\n",
    "         {'volume' : ['volume_w5_min', 'volume_w14_min', 'volume_w70_min']},\n",
    "         {'tmos_close' : ['tmos_close_w5_min', 'tmos_close_w14_min', 'tmos_close_w70_min']},\n",
    "    \n",
    "         {'close' : ['close_w5_max', 'close_w14_max', 'close_w70_max']},\n",
    "         {'volume' : ['volume_w5_max', 'volume_w14_max', 'volume_w70_max']},\n",
    "         {'tmos_close' : ['tmos_close_w5_max', 'tmos_close_w14_max', 'tmos_close_w70_max']},\n",
    "\n",
    "\n",
    "        #w5\n",
    "         {'close' : [  'close_w5_ma_low_2std', 'close_w5_ma_up_2std', 'close_w5_ma_low_3std', 'close_w5_ma_up_3std']},\n",
    "         {'volume' : [  'volume_w5_ma_low_2std', 'volume_w5_ma_up_2std', 'volume_w5_ma_low_3std', 'volume_w5_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_w5_ma_low_2std', 'tmos_close_w5_ma_up_2std', 'tmos_close_w5_ma_low_3std', 'tmos_close_w5_ma_up_3std']},\n",
    "        #w14\n",
    "         {'close' : [ 'close_w14_ma_low_2std', 'close_w14_ma_up_2std', 'close_w14_ma_low_3std', 'close_w14_ma_up_3std']},\n",
    "         {'volume' : [ 'volume_w14_ma_low_2std', 'volume_w14_ma_up_2std', 'volume_w14_ma_low_3std', 'volume_w14_ma_up_3std']},\n",
    "         {'tmos_close' : [ 'tmos_close_w14_ma_low_2std', 'tmos_close_w14_ma_up_2std', 'tmos_close_w14_ma_low_3std', 'tmos_close_w14_ma_up_3std']},\n",
    "        #w70\n",
    "         {'close' : [ 'close_w70_ma_low_2std', 'close_w70_ma_up_2std', 'close_w70_ma_low_3std', 'close_w70_ma_up_3std']},\n",
    "         {'volume' : [ 'volume_w70_ma_low_2std', 'volume_w70_ma_up_2std', 'volume_w70_ma_low_3std', 'volume_w70_ma_up_3std']},\n",
    "         {'tmos_close' : [ 'tmos_close_w70_ma_low_2std', 'tmos_close_w70_ma_up_2std', 'tmos_close_w70_ma_low_3std', 'tmos_close_w70_ma_up_3std']},\n",
    "\n",
    "\n",
    "    #comment\n",
    "        #w5\n",
    "        ['close_w5_min', 'close_w5_max'],\n",
    "        ['volume_w5_min', 'volume_w5_max'],\n",
    "        ['tmos_close_w5_min', 'tmos_close_w5_max'],\n",
    "        #w14\n",
    "        ['close_w14_min', 'close_w14_max'],\n",
    "        ['volume_w14_min', 'volume_w14_max'],\n",
    "        ['tmos_close_w14_min', 'tmos_close_w14_max'],\n",
    "        #w70\n",
    "        ['close_w70_min', 'close_w70_max'],\n",
    "        ['volume_w70_min', 'volume_w70_max'],\n",
    "        ['tmos_close_w70_min', 'tmos_close_w70_max'],\n",
    "]\n",
    "\n",
    "GROUPS_1DAY = [\n",
    "        #1day: #w1, #w3, #w5, #w20, #w100\n",
    "         ['close_1day_w1_roc', 'close_1day_w3_alpha', 'close_1day_w5_alpha', 'close_1day_w20_alpha', 'close_1day_w100_alpha'],\n",
    "         ['volume_1day_w1_roc','volume_1day_w3_alpha', 'volume_1day_w5_alpha', 'volume_1day_w20_alpha', 'volume_1day_w100_alpha'],\n",
    "         ['tmos_close_1day_w1_roc', 'tmos_close_1day_w3_alpha', 'tmos_close_1day_w5_alpha', 'tmos_close_1day_w20_alpha', 'tmos_close_1day_w100_alpha'],\n",
    "\n",
    "         ['close_1day_w1_roc', 'close_1day_w3_roc', 'close_1day_w5_roc', 'close_1day_w20_roc', 'close_1day_w100_roc'],\n",
    "         ['volume_1day_w1_roc', 'volume_1day_w3_roc', 'volume_1day_w5_roc', 'volume_1day_w20_roc', 'volume_1day_w100_roc'],\n",
    "         ['tmos_close_1day_w1_roc','tmos_close_1day_w3_roc', 'tmos_close_1day_w5_roc', 'tmos_close_1day_w20_roc', 'tmos_close_1day_w100_roc'],\n",
    "\n",
    "         ['close_1day_w3_mean_abs_pct', 'close_1day_w5_mean_abs_pct', 'close_1day_w20_mean_abs_pct', 'close_1day_w100_mean_abs_pct'],\n",
    "         ['volume_1day_w3_mean_abs_pct','volume_1day_w5_mean_abs_pct', 'volume_1day_w20_mean_abs_pct', 'volume_1day_w100_mean_abs_pct'],\n",
    "         ['tmos_close_1day_w3_mean_abs_pct', 'tmos_close_1day_w5_mean_abs_pct', 'tmos_close_1day_w20_mean_abs_pct', 'tmos_close_1day_w100_mean_abs_pct'],\n",
    "\n",
    "         ['close_1day_w3_std', 'close_1day_w5_std', 'close_1day_w20_std', 'close_1day_w100_std'],\n",
    "         ['volume_1day_w3_std', 'volume_1day_w5_std', 'volume_1day_w20_std', 'volume_1day_w100_std'],\n",
    "         ['tmos_close_1day_w3_std', 'tmos_close_1day_w5_std', 'tmos_close_1day_w20_std', 'tmos_close_1day_w100_std'],\n",
    "\n",
    "         ['close_1day_w3_norm_std', 'close_1day_w5_norm_std', 'close_1day_w20_norm_std', 'close_1day_w100_norm_std'],\n",
    "         ['volume_1day_w3_norm_std', 'volume_1day_w5_norm_std', 'volume_1day_w20_norm_std', 'volume_1day_w100_norm_std'],\n",
    "         ['tmos_close_1day_w3_norm_std', 'tmos_close_1day_w5_norm_std', 'tmos_close_1day_w20_norm_std', 'tmos_close_1day_w100_norm_std'],\n",
    "\n",
    "         ['close_1day_w3_rsi',  'close_1day_w5_rsi', 'close_1day_w20_rsi', 'close_1day_w100_rsi'],\n",
    "         ['volume_1day_w3_rsi', 'volume_1day_w5_rsi', 'volume_1day_w20_rsi', 'volume_1day_w100_rsi'],\n",
    "         ['tmos_close_1day_w3_rsi', 'tmos_close_1day_w5_rsi', 'tmos_close_1day_w20_rsi', 'tmos_close_1day_w100_rsi'],\n",
    "        \n",
    "         ['close_1day','close_1day_w3_ma', 'close_1day_w5_ma', 'close_1day_w20_ma', 'close_1day_w100_ma'],\n",
    "         ['volume_1day', 'volume_1day_w3_ma', 'volume_1day_w5_ma', 'volume_1day_w20_ma', 'volume_1day_w100_ma'],\n",
    "         ['tmos_close_1day','tmos_close_1day_w3_ma', 'tmos_close_1day_w5_ma', 'tmos_close_1day_w20_ma', 'tmos_close_1day_w100_ma'],\n",
    "\n",
    "         {'close_1day' : ['close_1day_w3_min', 'close_1day_w5_min', 'close_1day_w20_min', 'close_1day_w100_min']},\n",
    "         {'volume_1day' : ['volume_1day_w3_min', 'volume_1day_w5_min', 'volume_1day_w20_min', 'volume_1day_w100_min']},\n",
    "         {'tmos_close_1day' : ['tmos_close_1day_w3_min', 'tmos_close_1day_w5_min', 'tmos_close_1day_w20_min', 'tmos_close_1day_w100_min']},\n",
    "    \n",
    "         {'close_1day' : ['close_1day_w3_max', 'close_1day_w5_max', 'close_1day_w20_max', 'close_1day_w100_max']},\n",
    "         {'volume_1day' : ['volume_1day_w3_max', 'volume_1day_w5_max', 'volume_1day_w20_max', 'volume_1day_w100_max']},\n",
    "         {'tmos_close_1day' : ['tmos_close_1day_w3_max','tmos_close_1day_w5_max', 'tmos_close_1day_w20_max', 'tmos_close_1day_w100_max']},\n",
    "\n",
    "        #w3\n",
    "         {'close_1day' : [  'close_1day_w3_ma_low_2std', 'close_1day_w3_ma_up_2std', 'close_1day_w3_ma_low_3std', 'close_1day_w3_ma_up_3std']},\n",
    "         {'volume_1day' : [  'volume_1day_w3_ma_low_2std', 'volume_1day_w3_ma_up_2std', 'volume_1day_w3_ma_low_3std', 'volume_1day_w3_ma_up_3std']},\n",
    "         {'tmos_close_1day' : [  'tmos_close_1day_w3_ma_low_2std', 'tmos_close_1day_w3_ma_up_2std', 'tmos_close_1day_w3_ma_low_3std', 'tmos_close_1day_w3_ma_up_3std']},\n",
    "        #w5\n",
    "         {'close_1day' : [  'close_1day_w5_ma_low_2std', 'close_1day_w5_ma_up_2std', 'close_1day_w5_ma_low_3std', 'close_1day_w5_ma_up_3std']},\n",
    "         {'volume_1day' : [  'volume_1day_w5_ma_low_2std', 'volume_1day_w5_ma_up_2std', 'volume_1day_w5_ma_low_3std', 'volume_1day_w5_ma_up_3std']},\n",
    "         {'tmos_close_1day' : [ 'tmos_close_1day_w5_ma_low_2std', 'tmos_close_1day_w5_ma_up_2std', 'tmos_close_1day_w5_ma_low_3std', 'tmos_close_1day_w5_ma_up_3std']},\n",
    "        #w20\n",
    "         {'close_1day' : [ 'close_1day_w20_ma_low_2std', 'close_1day_w20_ma_up_2std', 'close_1day_w20_ma_low_3std', 'close_1day_w20_ma_up_3std']},\n",
    "         {'volume_1day' : [ 'volume_1day_w20_ma_low_2std', 'volume_1day_w20_ma_up_2std', 'volume_1day_w20_ma_low_3std', 'volume_1day_w20_ma_up_3std']},\n",
    "         {'tmos_close_1day' : [ 'tmos_close_1day_w20_ma_low_2std', 'tmos_close_1day_w20_ma_up_2std', 'tmos_close_1day_w20_ma_low_3std', 'tmos_close_1day_w20_ma_up_3std']},\n",
    "        #w100\n",
    "         {'close_1day' : [ 'close_1day_w100_ma_low_2std', 'close_1day_w100_ma_up_2std', 'close_1day_w100_ma_low_3std', 'close_1day_w100_ma_up_3std']},\n",
    "         {'volume_1day' : [ 'volume_1day_w100_ma_low_2std', 'volume_1day_w100_ma_up_2std', 'volume_1day_w100_ma_low_3std', 'volume_1day_w100_ma_up_3std']},\n",
    "         {'tmos_close_1day' : ['tmos_close_1day_w100_ma_low_2std', 'tmos_close_1day_w100_ma_up_2std', 'tmos_close_1day_w100_ma_low_3std', 'tmos_close_1day_w100_ma_up_3std']},\n",
    "\n",
    "    #comment\n",
    "        #w3\n",
    "        ['close_1day_w3_min', 'close_1day_w3_max'],\n",
    "        ['volume_1day_w3_min', 'volume_1day_w3_max'],\n",
    "        ['tmos_close_1day_w3_min', 'tmos_close_1day_w3_max'],    \n",
    "        #w5\n",
    "        ['close_1day_w5_min', 'close_1day_w5_max'],\n",
    "        ['volume_1day_w5_min', 'volume_1day_w5_max'],\n",
    "        ['tmos_close_1day_w5_min', 'tmos_close_1day_w5_max'],\n",
    "    \n",
    "        #w20\n",
    "        ['close_1day_w20_min', 'close_1day_w20_max'],\n",
    "        ['volume_1day_w20_min', 'volume_1day_w20_max'],\n",
    "        ['tmos_close_1day_w20_min', 'tmos_close_1day_w20_max'],\n",
    "    \n",
    "        # #w100 - вот эти фичи бы удалить - это скорее лик\n",
    "        # ['close_1day_w100_min', 'close_1day_w100_max'],\n",
    "        # ['volume_1day_w100_min', 'volume_1day_w100_max'],\n",
    "        # ['tmos_close_1day_w100_min', 'tmos_close_1day_w100_max'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868d0b9-4acc-4efc-815d-b897b4ae5cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af67e237-6c67-413d-b554-62bc82eea6c6",
   "metadata": {},
   "source": [
    "# 2. Make dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db61d334-729c-499c-8c88-94699c9152c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c324a1bc-adc1-48bd-8d78-26a255c13733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/raw: File exists\n",
      "mkdir: data/preproc: File exists\n",
      "mkdir: data/lgbm: File exists\n",
      "mkdir: data/result: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/raw data/preproc data/lgbm data/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06aa25cd-e957-472e-b78a-06bf1ce4b53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/raw/1day: File exists\n",
      "mkdir: data/raw/1hour: File exists\n",
      "mkdir: data/preproc/1day: File exists\n",
      "mkdir: data/preproc/1hour: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/raw/1day data/raw/1hour data/preproc/1day data/preproc/1hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c9d3e-35d2-4c61-a061-db9d7da21b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6f18d-b4fc-430c-a2e9-16962ecfc2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f85319c2-7e5a-44cf-a237-e3c8cd2c5035",
   "metadata": {},
   "source": [
    "# 2. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b404f1ca-4197-4da0-ad8b-0fac8c925744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_logs(text):\n",
    "    with open(\"logs.txt\", \"a\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde59a8-b20d-416d-8559-e37c3cb3c667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3357b12-dfcc-473f-a6e2-e4e760960312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_figi():\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start loading figi data' +'\\n')\n",
    "    #make dir\n",
    "\n",
    "    count_tries = 0\n",
    "    max_count_tries = 5\n",
    "    flag_data = False\n",
    "    while (not flag_data) and (count_tries < max_count_tries):\n",
    "        count_tries += 1\n",
    "        try:\n",
    "            with Client(TOKEN) as client:\n",
    "                instruments: InstrumentsService = client.instruments\n",
    "                tickers = []\n",
    "                for method in [\"shares\", \"bonds\", \"etfs\", \"currencies\", \"futures\"]:\n",
    "                    for item in getattr(instruments, method)().instruments:\n",
    "                        tickers.append(\n",
    "                            {\n",
    "                                \"name\": item.name,\n",
    "                                \"ticker\": item.ticker,\n",
    "                                \"class_code\": item.class_code,\n",
    "                                \"figi\": item.figi,\n",
    "                                \"uid\": item.uid,\n",
    "                                \"type\": method,\n",
    "                                \"min_price_increment\": quotation_to_decimal(\n",
    "                                    item.min_price_increment\n",
    "                                ),\n",
    "                                \"scale\": 9 - len(str(item.min_price_increment.nano)) + 1,\n",
    "                                \"lot\": item.lot,\n",
    "                                \"trading_status\": str(\n",
    "                                    SecurityTradingStatus(item.trading_status).name\n",
    "                                ),\n",
    "                                \"api_trade_available_flag\": item.api_trade_available_flag,\n",
    "                                \"currency\": item.currency,\n",
    "                                \"exchange\": item.exchange,\n",
    "                                \"buy_available_flag\": item.buy_available_flag,\n",
    "                                \"sell_available_flag\": item.sell_available_flag,\n",
    "                                \"short_enabled_flag\": item.short_enabled_flag,\n",
    "                                \"klong\": quotation_to_decimal(item.klong),\n",
    "                                \"kshort\": quotation_to_decimal(item.kshort),\n",
    "                            }\n",
    "                        )\n",
    "        \n",
    "            tickers_df = pd.DataFrame(tickers)\n",
    "            dump_pkl(tickers_df, './data/data_figi.pkl')\n",
    "            \n",
    "            flag_data = True\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "\n",
    "    if not flag_data:\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'Failed to load figi data' +'\\n')\n",
    "\n",
    "\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End loading figi data' +'\\n')\n",
    "\n",
    "def get_figi(ticker):\n",
    "\n",
    "        df_figi = load_pkl('./data/data_figi.pkl')\n",
    "\n",
    "        df_figi = df_figi[df_figi[\"ticker\"] == ticker]\n",
    "        if ticker == 'T': # таких два тикера T\n",
    "            df_figi = df_figi[df_figi['currency'] == 'rub']\n",
    "            \n",
    "        assert df_figi.shape[0] == 1, f\"{ticker}: error figi\"\n",
    "\n",
    "        figi = df_figi[\"figi\"].iloc[0]\n",
    "        return figi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e3131-09a5-415c-ae1d-4d25ef2b3ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0982e974-be2c-47ee-be68-9564e4afba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_candles(ticker=None, from_=None, to_=None, interval=None):\n",
    "    \n",
    "    figi = get_figi(ticker)\n",
    "    \n",
    "    with Client(TOKEN) as client:\n",
    "        \n",
    "        data = [elem for elem in client.get_all_candles(figi=figi, \n",
    "                                      from_=from_-timedelta(hours=3),\n",
    "                                      to=to_- timedelta(hours=3),\n",
    "                                     interval=interval\n",
    "                                    )\n",
    "               ]\n",
    "\n",
    "    \n",
    "    open_ = np.zeros(len(data))\n",
    "    high = np.zeros(len(data))\n",
    "    low = np.zeros(len(data))\n",
    "    close = np.zeros(len(data))\n",
    "    volume = np.zeros(len(data))\n",
    "    time = np.zeros(len(data), dtype='O')\n",
    "    for i, elem in enumerate(data):\n",
    "        open_[i] = quotation_to_decimal(elem.open)\n",
    "        high[i] = quotation_to_decimal(elem.high)\n",
    "        low[i] = quotation_to_decimal(elem.low)\n",
    "        close[i] = quotation_to_decimal(elem.close)\n",
    "        volume[i] = elem.volume\n",
    "        time[i] = elem.time\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'time' : time,\n",
    "                      'open' : open_,\n",
    "                      'close' : close,\n",
    "                      'volume' : volume,\n",
    "                      'low' : low,\n",
    "                      'high' : high\n",
    "                    })\n",
    "    df['time'] += pd.Timedelta(hours=3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda60da4-66e9-4219-8d33-8501fa46f9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6b1cf-008a-4e87-b2bc-04601ff77e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af7102-696a-44dd-9e38-c19fb72eebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e21c47f3-75e4-405b-8bc2-ffa570d7a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "#load_figi()\n",
    "\n",
    "# ticker = 'SFIN'\n",
    "# #2024-06-10 \n",
    "# data = get_all_candles(ticker=ticker,\n",
    "#                        from_=datetime.datetime(2024, 12, 29, tzinfo=datetime.timezone.utc),\n",
    "#                        to_=datetime.datetime(2025, 1, 29, tzinfo=datetime.timezone.utc),\n",
    "#                        interval=CandleInterval.CANDLE_INTERVAL_HOUR\n",
    "#                       )\n",
    "\n",
    "# data['ticker'] = ticker\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9284a6a8-260c-476b-aeac-7d0cb2662573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "#load_figi()\n",
    "\n",
    "\n",
    "# ticker = 'SFIN'\n",
    "# #2024-06-10 \n",
    "# data = get_all_candles(ticker=ticker,\n",
    "#                        from_=datetime.datetime(2025, 1, 20, tzinfo=datetime.timezone.utc),\n",
    "#                        to_=datetime.datetime(2025, 1, 29, tzinfo=datetime.timezone.utc),\n",
    "#                        interval=CandleInterval.CANDLE_INTERVAL_DAY\n",
    "#                       )\n",
    "\n",
    "# data['ticker'] = ticker\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3650dc5-409c-47f3-9966-a027d2c4dd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135fe20-116a-4ac9-8b58-cba86f47c24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1b4b2-f535-4ed9-906f-1e0545934073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bca12996-30bd-4d35-b359-4485d996a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_business_day(date):\n",
    "    return bool(len(pd.bdate_range(date, date)))\n",
    "\n",
    "\n",
    "def get_data_1day(stocks):\n",
    "    #loading figi\n",
    "    load_figi()\n",
    "\n",
    "    \n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start loading daily data' +'\\n')    \n",
    "    \n",
    "    #make dir\n",
    "    time_name = str(pd.Timestamp.now().date()) \n",
    "    if not os.path.exists(f'./data/raw/1day/{time_name}'):\n",
    "        os.mkdir(f'./data/raw/1day/{time_name}')\n",
    "\n",
    "    #loading data\n",
    "    end = pd.Timestamp.now()+pd.Timedelta(days=1) #+1day, чтобы выгрузить все, что есть (+1 точка для проверки актуальности данных)\n",
    "    start = end - pd.Timedelta(days=200)\n",
    "    for ticker in tqdm(stocks+['TMOS']): \n",
    "        \n",
    "        count_tries = 0\n",
    "        max_count_tries = 3\n",
    "        flag_data = False\n",
    "        while (not flag_data) and (count_tries < max_count_tries):\n",
    "            count_tries += 1\n",
    "            try:\n",
    "                data = get_all_candles(ticker=ticker,\n",
    "                               from_=start,\n",
    "                               to_=end,\n",
    "                               interval=CandleInterval.CANDLE_INTERVAL_DAY)\n",
    "                flag_data = True\n",
    "            except:\n",
    "                time.sleep(3)    \n",
    "        #saving\n",
    "        if flag_data:\n",
    "            data = data[['time', 'close', 'volume']]\n",
    "            data['ticker'] = ticker\n",
    "            dump_pkl(data, f'./data/raw/1day/{time_name}/{ticker}.pkl')\n",
    "        else:\n",
    "            write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'{ticker}: failed to load daily data' +'\\n')\n",
    "            \n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End loading daily data' +'\\n')\n",
    "\n",
    "\n",
    "\n",
    "def get_data_1hour(stocks):\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start loading hourly data' +'\\n')\n",
    "\n",
    "    #make dir\n",
    "    time_name = str(pd.Timestamp.now()).split(':')[0].replace(' ', '_') #date+hour\n",
    "    if not os.path.exists(f'./data/raw/1hour/{time_name}'):\n",
    "        os.mkdir(f'./data/raw/1hour/{time_name}')\n",
    "    \n",
    "    #loading data\n",
    "    end = pd.Timestamp.now()+pd.Timedelta(days=1) #+1day, чтобы выгрузить все, что есть (+1 точка для проверки целостности данных)\n",
    "    start = end - pd.Timedelta(days=15)\n",
    "    for ticker in tqdm(stocks+['TMOS']): \n",
    "        \n",
    "        count_tries = 0\n",
    "        max_count_tries = 3\n",
    "        flag_data = False\n",
    "        while (not flag_data) and (count_tries < max_count_tries):\n",
    "            count_tries += 1\n",
    "            try:\n",
    "                data = get_all_candles(ticker=ticker,\n",
    "                               from_=start,\n",
    "                               to_=end,\n",
    "                               interval=CandleInterval.CANDLE_INTERVAL_HOUR)\n",
    "                flag_data = True\n",
    "            except:\n",
    "                time.sleep(3)    \n",
    "        #saving\n",
    "        if flag_data:\n",
    "            data = data[['time', 'close', 'volume']]\n",
    "            data['ticker'] = ticker\n",
    "            dump_pkl(data, f'./data/raw/1hour/{time_name}/{ticker}.pkl')\n",
    "        else:\n",
    "            write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'{ticker}: failed to load hourly data' +'\\n')\n",
    "\n",
    "    \n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End loading hourly data' +'\\n')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238746e3-02b1-4887-8e5b-80ece1bfd832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9293e88d-1071-4c0a-81ae-61004893bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data_1day(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfaf9f54-c0d2-4441-90e7-848f11d0f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data_1hour(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dc30535-b352-460b-88e6-824f369d6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_pkl('./data/raw/1hour/2025-01-29_14/NLMK.pkl')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ed123-5f5b-48f2-9351-3f40063715bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9b77f48-e902-49b2-b275-74e974a2e3bb",
   "metadata": {},
   "source": [
    "# 4. Preproc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc865009-b381-4e43-9042-423c12a8142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_business_day(date):\n",
    "    return bool(len(pd.bdate_range(date, date)))\n",
    "\n",
    "def time_cut_data_1day(data):\n",
    "    data['time'] = data['time'].apply(lambda x: x.tz_convert(None))\n",
    "    \n",
    "    mask_bd = np.array(data['time'].apply(lambda x: is_business_day(x)))\n",
    "    data = data.loc[mask_bd, :]\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def preproc_data_1day(stocks):\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start preproc daily data' +'\\n')\n",
    "\n",
    "    #make dir\n",
    "    time_name = str(pd.Timestamp.now().date()) \n",
    "    if not os.path.exists(f'./data/preproc/1day/{time_name}'):\n",
    "        os.mkdir(f'./data/preproc/1day/{time_name}')\n",
    "\n",
    "    #tmos\n",
    "    if not os.path.exists(f\"./data/raw/1day/{time_name}/TMOS.pkl\"):\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'TMOS: failed to preproc daily data' +'\\n')\n",
    "        return\n",
    "    data_tmos = load_pkl(f\"./data/raw/1day/{time_name}/TMOS.pkl\")\n",
    "\n",
    "    #preproc\n",
    "    for ticker in tqdm(stocks):\n",
    "        try:\n",
    "            data = load_pkl(f\"./data/raw/1day/{time_name}/{ticker}.pkl\")\n",
    "        \n",
    "            data_ext = data.merge(data_tmos[['time', 'close']].copy().rename(columns={\"close\" : \"tmos_close\"}), how='left', on='time')\n",
    "            assert data.shape[0] == data_ext.shape[0], 'Error join tmos'\n",
    "            \n",
    "            data = time_cut_data_1day(data_ext)\n",
    "            assert data['tmos_close'].isnull().sum() == 0, 'Error tmos_close nulls' #праздники, в них индекс не торгуется (мб в послед время рассчитывется?)\n",
    "            data['tmos_close'] = data['tmos_close'].ffill()\n",
    "\n",
    "            #Actual time date (не самая идеальная реализация, так как чекает наличие следующую точку после прогнозной, а не прогнозную)\n",
    "            last_time = data['time'].iloc[-1]\n",
    "            assert last_time.date() == pd.Timestamp.now().date(), 'Error time_data' \n",
    "            \n",
    "            #Обрежем последнюю точку \n",
    "            mask = data['time'] < pd.Timestamp.now().floor('d')\n",
    "            data = data[mask]\n",
    "            \n",
    "            data = data.iloc[-(COUNT_DAY_POINTS+1):]\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            #len\n",
    "            assert data.shape[0] == (COUNT_DAY_POINTS+1), 'Error len'\n",
    "            \n",
    "            #Заполняемость\n",
    "            diff_time_day = data['time'].diff() / pd.Timedelta(hours=24)\n",
    "            assert (diff_time_day.iloc[1:] % 1 == 0).all(), 'Error filling 1'\n",
    "            assert (diff_time_day.iloc[1:] == 2).sum() < 5, 'Error filling 2'\n",
    "            # assert (diff_time_day.iloc[1:] < 10).all(), 'Error 5' #переезд акций ломает тест\n",
    "\n",
    "            #duplicates in time\n",
    "            assert (diff_time_day != 0).all(), 'Error time duplicates'\n",
    "\n",
    "            #sort in time\n",
    "            assert (diff_time_day.iloc[1:] > 0).all(), 'Error time sort'\n",
    "\n",
    "            #Nulls\n",
    "            assert not data.isnull().any().any(), 'Error nulls'  \n",
    "            \n",
    "            dump_pkl(data, f'./data/preproc/1day/{time_name}/{ticker}.pkl')\n",
    "        except Exception as exception:\n",
    "            write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'{ticker}: failed to preproc daily data ({exception})' +'\\n')\n",
    "\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End preproc daily data' +'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def time_cut_data_1hour(data):\n",
    "    data['time'] = data['time'].apply(lambda x: x.tz_convert(None))\n",
    "    \n",
    "    mask_bd = np.array(data['time'].apply(lambda x: is_business_day(x)))\n",
    "    data = data.loc[mask_bd, :]\n",
    "\n",
    "    mask_volume = np.array(datetime.time(10, 0) <= pd.to_datetime(data['time'], format='%H:%M').dt.time)\n",
    "    data = data.loc[mask_volume, :]\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def preproc_data_1hour(stocks):\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start preproc hourly data' +'\\n')\n",
    "\n",
    "    #make dir\n",
    "    time_name = str(pd.Timestamp.now()).split(':')[0].replace(' ', '_') #date+hour\n",
    "    if not os.path.exists(f'./data/preproc/1hour/{time_name}'):\n",
    "        os.mkdir(f'./data/preproc/1hour/{time_name}')\n",
    "\n",
    "    #tmos\n",
    "    if not os.path.exists(f\"./data/raw/1hour/{time_name}/TMOS.pkl\"):\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'TMOS: failed to preproc hourly data' +'\\n')\n",
    "        return\n",
    "    data_tmos = load_pkl(f\"./data/raw/1hour/{time_name}/TMOS.pkl\")\n",
    "    #костыль\n",
    "    if data_tmos['time'].iloc[-1] == data_tmos['time'].iloc[-2]:\n",
    "        data_tmos = data_tmos.iloc[:-1].reset_index(drop=True)\n",
    "\n",
    "    #preproc\n",
    "    for ticker in tqdm(stocks):\n",
    "        try:\n",
    "            data = load_pkl(f\"./data/raw/1hour/{time_name}/{ticker}.pkl\")\n",
    "        \n",
    "            data_ext = data.merge(data_tmos[['time', 'close']].copy().rename(columns={\"close\" : \"tmos_close\"}), how='left', on='time')\n",
    "            assert data.shape[0] == data_ext.shape[0], 'Error join tmos'\n",
    "        \n",
    "            data = time_cut_data_1hour(data_ext)\n",
    "            assert data['tmos_close'].isnull().sum() < 50, 'Error tmos_close nulls' #праздники, в них индекс не торгуется\n",
    "            data['tmos_close'] = data['tmos_close'].ffill()\n",
    "\n",
    "            #Actual time date (не самая идеальная реализация, так как чекает наличие следующую точку после прогнозной, а не прогнозную)\n",
    "            last_time = data['time'].iloc[-1]\n",
    "            assert (last_time == pd.Timestamp.now().floor('h')) or\\\n",
    "                            (last_time == (pd.Timestamp.now() - pd.Timedelta(hours=1)).floor('h')), 'Error time_data' #послабление для низколиквидных акций\n",
    "            \n",
    "            #Обрежем последнюю точку (или две, они бывают задваиваются - особенность онлайн данных)\n",
    "            mask = data['time'] < pd.Timestamp.now().floor('h')\n",
    "            data = data[mask]\n",
    "            \n",
    "            data = data.iloc[-(COUNT_HOUR_POINTS+1):]\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            #len\n",
    "            assert data.shape[0] == (COUNT_HOUR_POINTS+1), 'Error len'\n",
    "            \n",
    "            #Заполняемость\n",
    "            diff_time_hour = data['time'].diff() / pd.Timedelta(hours=1)\n",
    "            assert (diff_time_hour.iloc[1:] % 1 == 0).all(), 'Error filling 1'\n",
    "            assert (diff_time_hour.iloc[1:] == 2).sum() < 10, 'Error filling 2'\n",
    "            assert (diff_time_hour.iloc[1:] < 24*5).all(), 'Error filling 3' #переезд акций\n",
    "\n",
    "            #duplicates in time\n",
    "            assert (diff_time_hour != 0).all(), 'Error time duplicates'\n",
    "\n",
    "            #sort in time\n",
    "            assert (diff_time_hour.iloc[1:] > 0).all(), 'Error time sort'\n",
    "            \n",
    "            #Nulls\n",
    "            assert not data.isnull().any().any(), 'Error nulls'\n",
    "    \n",
    "        \n",
    "            dump_pkl(data, f'./data/preproc/1hour/{time_name}/{ticker}.pkl')\n",
    "        except Exception as exception:\n",
    "            write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'{ticker}: failed to preproc hourly data ({exception})' +'\\n')\n",
    "\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End preproc hourly data' +'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd6ffe1-b11d-4fc3-bdde-73fae5d753e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70f4a3c3-fe98-4784-828b-f60f22bcba15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preproc_data_1day(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f3318f1-f600-4ddd-956a-5fb77f80954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc_data_1hour(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea02b25-87f4-4271-8556-8f93b566c9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21f7c648-2ac1-45c8-96bc-d5214063d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_pkl('./data/preproc/1day/2025-01-29/YDEX.pkl')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "196d5ff7-df23-4b35-ad99-0074b8e5f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_pkl('./data/raw/1hour/2025-02-03_10/AKRN.pkl')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee7fba29-c611-4693-83e3-099c3fad7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data_1hour(STOCKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99289433-2efc-4b9b-9117-d404afcb7365",
   "metadata": {},
   "source": [
    "# 5. Prepare data to lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51b9f6bc-3955-4efd-81e0-6c81b27f7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def calculate_bollinger_bands(data, window):\n",
    "    \"\"\"Calculate Bollinger Bands\"\"\"\n",
    "    rolling_mean = data.rolling(window=window, min_periods=1).mean().values\n",
    "    rolling_std = data.rolling(window=window, min_periods=1).std().values\n",
    "\n",
    "    norm_rolling_std = rolling_std/rolling_mean\n",
    "\n",
    "    num_of_std = 2\n",
    "    lower_band_2std = rolling_mean - (rolling_std * num_of_std)\n",
    "    upper_band_2std = rolling_mean + (rolling_std * num_of_std)\n",
    "    \n",
    "    num_of_std = 3\n",
    "    lower_band_3std = rolling_mean - (rolling_std * num_of_std)\n",
    "    upper_band_3std = rolling_mean + (rolling_std * num_of_std)\n",
    "    \n",
    "    \n",
    "    return rolling_mean, rolling_std, norm_rolling_std, lower_band_2std, upper_band_2std, lower_band_3std, upper_band_3std\n",
    "\n",
    "def calculate_rsi(data, window):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    delta = data.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    mask = avg_loss == 0\n",
    "    rsi[mask] = 100\n",
    "    \n",
    "    return rsi.values\n",
    "\n",
    "def calculate_roc(data, periods):\n",
    "    \"\"\"Calculate Rate of Change.\"\"\"\n",
    "    roc = ((data - data.shift(periods)) / data.shift(periods))\n",
    "    return roc.values\n",
    "\n",
    "\n",
    "\n",
    "def calc_stats(data, window=None, feat_name=None):\n",
    "    #mean, std\n",
    "    rolling_mean, rolling_std, norm_rolling_std,\\\n",
    "    lower_band_2std, upper_band_2std, lower_band_3std, upper_band_3std = calculate_bollinger_bands(data, window)\n",
    "\n",
    "    #mean_abs_pct\n",
    "    mean_abs_pct = data.pct_change(periods=1).rolling(window=window, min_periods=1).apply(lambda x: x.abs().mean())\n",
    "        \n",
    "    #alpha\n",
    "    alpha = data.rolling(window=window, min_periods=2).apply(lambda x: LinearRegression().fit(x.values.reshape(-1, 1), np.arange(x.shape[0])).coef_[0])\n",
    "\n",
    "    #min, max\n",
    "    rolling_min = data.rolling(window=window, min_periods=1).min().values\n",
    "    rolling_max = data.rolling(window=window, min_periods=1).max().values\n",
    "    \n",
    "    #rsi\n",
    "    rsi = calculate_rsi(data, window)\n",
    "    \n",
    "    #roc\n",
    "    roc = calculate_roc(data, window)\n",
    "    diff = data.diff(window).values\n",
    "\n",
    "    #можно угол угла наклона добавить, чтобы определять фазы рынка\n",
    "    \n",
    "    df_features = pd.DataFrame({f'{feat_name}_ma' : rolling_mean,\n",
    "                        f'{feat_name}_std' : rolling_std,\n",
    "                        f'{feat_name}_norm_std' : norm_rolling_std,\n",
    "                        f'{feat_name}_ma_low_2std' : lower_band_2std,\n",
    "                        f'{feat_name}_ma_up_2std' : upper_band_2std,\n",
    "                        f'{feat_name}_ma_low_3std' : lower_band_3std,\n",
    "                        f'{feat_name}_ma_up_3std' : upper_band_3std, \n",
    "\n",
    "                        f'{feat_name}_mean_abs_pct' : mean_abs_pct,\n",
    "                            \n",
    "                        f'{feat_name}_alpha' : alpha,\n",
    "                            \n",
    "                        f'{feat_name}_min' : rolling_min,\n",
    "                        f'{feat_name}_max' : rolling_max,\n",
    "                        f'{feat_name}_rsi' : rsi,\n",
    "                        f'{feat_name}_roc' : roc,\n",
    "                        f'{feat_name}_diff' : diff,\n",
    "                        })\n",
    "    return df_features\n",
    "\n",
    "\n",
    "def calc_stats_diff_1(data, feat_name=None):\n",
    "    return pd.DataFrame({f'{feat_name}_roc' : data.pct_change(periods=1).values,\n",
    "                        f'{feat_name}_diff' : data.diff(1).values,\n",
    "                        })\n",
    "\n",
    "def calc_levels(data, window=None, levels=None, feat_name=None):\n",
    "    \n",
    "    #уровни\n",
    "    data_levels = []\n",
    "    column_names = []\n",
    "    for i in range(1, len(levels)):\n",
    "        level_low = levels[i-1]\n",
    "        level_high = levels[i]\n",
    "        data_levels += [data.rolling(window=window, min_periods=1).apply(lambda x: (((1+level_low)*x.values[-1] < x.values) & (x.values <= (1+level_high)*x.values[-1])).sum()).values]\n",
    "        data_levels += [data.rolling(window=window, min_periods=1).apply(lambda x: (((1-level_high)*x.values[-1] <= x.values) & (x.values < (1-level_low)*x.values[-1])).sum()).values]\n",
    "\n",
    "        column_names += [f\"{feat_name}_lvl_{1+level_low}-{1+level_high}\"]\n",
    "        column_names += [f\"{feat_name}_lvl_-{1-level_high}-{1-level_low}\"]\n",
    "    df_levels = pd.DataFrame({column_names[i]:data_levels[i] for i in range(len(column_names))})\n",
    "    return df_levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3c971-9c63-453c-b3d8-9c2fd34feb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf66d9b5-f6a8-4894-a1b4-24beab1cb802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features_1day(df_ticker):\n",
    "    levels =      [0, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.07]\n",
    "    levels_tmos = [0, 0.005, 0.01, 0.015, 0.02, 0.03, 0.04, 0.05]\n",
    "\n",
    "    #w1\n",
    "    df_close_w1 = calc_stats_diff_1(df_ticker['close_1day'], feat_name='close_1day_w1')\n",
    "    df_volume_w1 = calc_stats_diff_1(df_ticker['volume_1day'], feat_name='volume_1day_w1')\n",
    "    df_tmos_close_w1 = calc_stats_diff_1(df_ticker['tmos_close_1day'], feat_name='tmos_close_1day_w1')\n",
    "    assert df_ticker.shape[0] == df_close_w1.shape[0] == df_volume_w1.shape[0] == df_tmos_close_w1.shape[0], 'Error w1'\n",
    "    \n",
    "    #w3\n",
    "    df_close_w3 = calc_stats(df_ticker['close_1day'], window=3, feat_name='close_1day_w3')\n",
    "    df_volume_w3 = calc_stats(df_ticker['volume_1day'], window=3, feat_name='volume_1day_w3')\n",
    "    df_tmos_close_w3 = calc_stats(df_ticker['tmos_close_1day'], window=3, feat_name='tmos_close_1day_w3')\n",
    "    assert df_ticker.shape[0] == df_close_w3.shape[0] == df_volume_w3.shape[0] == df_tmos_close_w3.shape[0], 'Error w3'\n",
    "    \n",
    "    #w5\n",
    "    df_close_w5 = calc_stats(df_ticker['close_1day'], window=5, feat_name='close_1day_w5')\n",
    "    df_volume_w5 = calc_stats(df_ticker['volume_1day'], window=5, feat_name='volume_1day_w5')\n",
    "    df_tmos_close_w5 = calc_stats(df_ticker['tmos_close_1day'], window=5, feat_name='tmos_close_1day_w5')\n",
    "    assert df_ticker.shape[0] == df_close_w5.shape[0] == df_volume_w5.shape[0] == df_tmos_close_w5.shape[0], 'Error w5'\n",
    "    \n",
    "    #w20\n",
    "    df_close_w20 = calc_stats(df_ticker['close_1day'], window=5*4, feat_name='close_1day_w20')\n",
    "    df_close_levels_w20 = calc_levels(df_ticker['close_1day'], window=5*4, levels=levels, feat_name='close_1day_w20')\n",
    "    df_volume_w20 = calc_stats(df_ticker['volume_1day'], window=5*4, feat_name='volume_1day_w20')\n",
    "    df_tmos_close_w20 = calc_stats(df_ticker['tmos_close_1day'], window=5*4, feat_name='tmos_close_1day_w20')\n",
    "    df_tmos_close_levels_w20 = calc_levels(df_ticker['tmos_close_1day'], window=5*4, levels=levels_tmos, feat_name='tmos_close_1day_w20')\n",
    "    assert df_ticker.shape[0] == df_close_w20.shape[0] == df_close_levels_w20.shape[0] == df_volume_w20.shape[0] == df_tmos_close_w20.shape[0] == df_tmos_close_levels_w20.shape[0], 'Error w20'\n",
    "    \n",
    "    \n",
    "    #w100\n",
    "    df_close_w100 = calc_stats(df_ticker['close_1day'], window=100, feat_name='close_1day_w100')\n",
    "    df_close_levels_w100 = calc_levels(df_ticker['close_1day'], window=100, levels=levels, feat_name='close_1day_w100')\n",
    "    df_volume_w100 = calc_stats(df_ticker['volume_1day'], window=100, feat_name='volume_1day_w100')\n",
    "    df_tmos_close_w100 = calc_stats(df_ticker['tmos_close_1day'], window=100, feat_name='tmos_close_1day_w100')\n",
    "    df_tmos_close_levels_w100 = calc_levels(df_ticker['tmos_close_1day'], window=100, levels=levels_tmos, feat_name='tmos_close_1day_w100')\n",
    "    assert df_ticker.shape[0] == df_close_w100.shape[0] == df_close_levels_w100.shape[0] == df_volume_w100.shape[0] == df_tmos_close_w100.shape[0] == df_tmos_close_levels_w100.shape[0], 'Error w100'\n",
    "    \n",
    "    df = pd.concat([df_ticker,\n",
    "                       df_close_w1, df_volume_w1, df_tmos_close_w1, \n",
    "                       df_close_w3, df_volume_w3, df_tmos_close_w3, \n",
    "                       df_close_w5, df_volume_w5, df_tmos_close_w5,\n",
    "                       df_close_w20, df_close_levels_w20, df_volume_w20, df_tmos_close_w20, df_tmos_close_levels_w20,\n",
    "                       df_close_w100, df_close_levels_w100, df_volume_w100, df_tmos_close_w100, df_tmos_close_levels_w100], axis=1)\n",
    "    \n",
    "    assert (df_ticker.shape[0] == df.shape[0]) and (df.shape[1] == (df_ticker.shape[1]+3*df_close_w1.shape[1]+12*df_close_w5.shape[1]+4*df_close_levels_w20.shape[1])), 'Error concat'\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_features_1hour(df_ticker):\n",
    "    \n",
    "    levels =      [0, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.07]\n",
    "    levels_tmos = [0, 0.005, 0.01, 0.015, 0.02, 0.03, 0.04, 0.05]\n",
    "\n",
    "    #w1\n",
    "    df_close_w1 = calc_stats_diff_1(df_ticker['close'], feat_name='close_w1')\n",
    "    df_volume_w1 = calc_stats_diff_1(df_ticker['volume'], feat_name='volume_w1')\n",
    "    df_tmos_close_w1 = calc_stats_diff_1(df_ticker['tmos_close'], feat_name='tmos_close_w1')\n",
    "    assert df_ticker.shape[0] == df_close_w1.shape[0] == df_volume_w1.shape[0] == df_tmos_close_w1.shape[0], 'Error w1'\n",
    "    \n",
    "    #w5\n",
    "    df_close_w5 = calc_stats(df_ticker['close'], window=5, feat_name='close_w5')\n",
    "    df_volume_w5 = calc_stats(df_ticker['volume'], window=5, feat_name='volume_w5')\n",
    "    df_tmos_close_w5 = calc_stats(df_ticker['tmos_close'], window=5, feat_name='tmos_close_w5')\n",
    "    assert df_ticker.shape[0] == df_close_w5.shape[0] == df_volume_w5.shape[0] == df_tmos_close_w5.shape[0], 'Error w5'\n",
    "    \n",
    "    #w14\n",
    "    df_close_w14 = calc_stats(df_ticker['close'], window=14, feat_name='close_w14')\n",
    "    df_volume_w14 = calc_stats(df_ticker['volume'], window=14, feat_name='volume_w14')\n",
    "    df_tmos_close_w14 = calc_stats(df_ticker['tmos_close'], window=14, feat_name='tmos_close_w14')\n",
    "    assert df_ticker.shape[0] == df_close_w14.shape[0] == df_volume_w14.shape[0] == df_tmos_close_w14.shape[0], 'Error w14'\n",
    "\n",
    "    #w70=14*5\n",
    "    df_close_w70 = calc_stats(df_ticker['close'], window=70, feat_name='close_w70')\n",
    "    df_close_levels_w70 = calc_levels(df_ticker['close'], window=70, levels=levels, feat_name=\"close_w70\")\n",
    "    df_volume_w70 = calc_stats(df_ticker['volume'], window=70, feat_name='volume_w70')\n",
    "    df_tmos_close_w70 = calc_stats(df_ticker['tmos_close'], window=70, feat_name='tmos_close_w70')\n",
    "    df_tmos_close_levels_w70 = calc_levels(df_ticker['tmos_close'], window=70, levels=levels_tmos, feat_name='tmos_close_w70')\n",
    "    assert df_ticker.shape[0] == df_close_w70.shape[0] == df_close_levels_w70.shape[0] == df_volume_w70.shape[0] == df_tmos_close_w70.shape[0] == df_tmos_close_levels_w70.shape[0], 'Error w70'\n",
    "\n",
    "\n",
    "    \n",
    "    df = pd.concat([df_ticker,\n",
    "                       df_close_w1, df_volume_w1, df_tmos_close_w1,\n",
    "                       df_close_w5, df_volume_w5, df_tmos_close_w5,\n",
    "                       df_close_w14, df_volume_w14, df_tmos_close_w14,\n",
    "                       df_close_w70, df_close_levels_w70, df_volume_w70, df_tmos_close_w70, df_tmos_close_levels_w70\n",
    "                      ], axis=1)\n",
    "    \n",
    "    assert (df_ticker.shape[0] == df.shape[0]) and (df.shape[1] == (df_ticker.shape[1]+3*df_close_w1.shape[1]+9*df_close_w5.shape[1]+2*df_close_levels_w70.shape[1])), 'Error concat'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba5e7c-b332-4cf4-acfe-77cd869b83a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6a2dd-ee24-49c4-afc4-105802ee7169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e00ac5-e53f-4041-99fb-188d276d6eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1de949ab-0cd6-4b9e-b640-69e6a2d6be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniq_pairs(cols):\n",
    "    pairs = []\n",
    "    for i in range(len(cols)-1):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            pairs += [(cols[i], cols[j])]\n",
    "    return pairs\n",
    "\n",
    "def calc_relative_features(df, groups):\n",
    "    for group in groups:\n",
    "        if type(group) == list:\n",
    "            pairs = uniq_pairs(group)\n",
    "            for pair in pairs:\n",
    "                new_col = f'{pair[0]}/{pair[1]}'\n",
    "                df[new_col] = df[pair[0]] / (df[pair[1]] + np.finfo(np.float32).eps)\n",
    "\n",
    "        if type(group) == dict:\n",
    "            pair1 = list(group.keys())[0]\n",
    "            for pair0 in group[pair1]:\n",
    "                new_col = f'{pair0}/{pair1}'\n",
    "                df[new_col] = df[pair0] / (df[pair1] + np.finfo(np.float32).eps)\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab4bc073-8c74-4e58-b182-cce9eb9ca666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_lgbm(stocks, features):\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start prepare data to lgbm' +'\\n')\n",
    "\n",
    "    day_name = str(pd.Timestamp.now().date()) \n",
    "    hour_name = str(pd.Timestamp.now()).split(':')[0].replace(' ', '_')\n",
    "\n",
    "    #prepare\n",
    "    data_lgbm = []\n",
    "\n",
    "    #тут можно ускорить на минуту, если в одном общем датафрейме считать фичи (часовый и дневные)\n",
    "    for ticker in tqdm(stocks):\n",
    "        try:\n",
    "            df_1day = load_pkl(f\"./data/preproc/1day/{day_name}/{ticker}.pkl\")\n",
    "            df_1hour = load_pkl(f\"./data/preproc/1hour/{hour_name}/{ticker}.pkl\")\n",
    "\n",
    "            df_1day = df_1day.rename(columns={col : col+'_1day' for col in df_1day.columns})            \n",
    "\n",
    "            # вот тут\n",
    "            df_1day = calculate_features_1day(df_1day).iloc[-1:].reset_index(drop=True)\n",
    "            df_1hour = calculate_features_1hour(df_1hour).iloc[-1:].reset_index(drop=True)\n",
    "\n",
    "            #time features\n",
    "            df_1hour['hour'] = df_1hour['time'].dt.hour\n",
    "            df_1hour['day'] = df_1hour['time'].dt.day\n",
    "            df_1hour['weekday'] = np.minimum(df_1hour['time'].dt.dayofweek, 4) / 4\n",
    "\n",
    "            assert not df_1day.isnull().any().any(), 'Error nulls df_1day'\n",
    "            assert not df_1hour.isnull().any().any(), 'Error nulls df_1hour'\n",
    "            \n",
    "            data = pd.concat([df_1hour, df_1day], axis=1)\n",
    "        \n",
    "            data_lgbm += [data.copy()]\n",
    "            \n",
    "        except Exception as exception:\n",
    "            write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'{ticker}: failed to prepare data to lgbm ({exception})' +'\\n')\n",
    "\n",
    "    try:\n",
    "        data_lgbm = pd.concat(data_lgbm)\n",
    "        data_lgbm = calc_relative_features(data_lgbm, GROUPS_1DAY+GROUPS_1HOUR)\n",
    "        \n",
    "        assert not data_lgbm.isnull().any().any(), 'Error nulls data_lgbm'  \n",
    "        \n",
    "        data_lgbm = data_lgbm[['time', 'ticker', 'close', 'time_1day'] + features].reset_index(drop=True).copy()\n",
    "        dump_pkl(data_lgbm, f\"./data/lgbm/data_lgbm_{hour_name}.pkl\")\n",
    "\n",
    "    except Exception as exception:\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'Failed to prepare data to lgbm ({exception})' +'\\n')\n",
    "\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End prepare data to lgbm' +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259cff2-304d-4762-97de-8f8d7a21b50d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc3413cc-568a-4007-b41e-61bc3c69b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare_data_lgbm(STOCKS, FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f22513-b882-47fe-bc66-2070d6a5b436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02414c22-f99f-49fa-a986-0ea4ac98dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = load_pkl('./data/lgbm/data_lgbm_2025-02-04_16.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd975ebe-ecd4-4958-a737-c851b03eb0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163a5db-1e83-4297-8938-281f0d2888d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36eac7e8-f4c1-41e8-8b47-73601e44b955",
   "metadata": {},
   "source": [
    "# 6. predict lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f48b9e36-6404-453b-b04f-92dd2c1af3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lgbm_model:\n",
    "    def __init__(self, name=None, strategy=None, threshold=None, path=None, features=None):\n",
    "        self.name = name\n",
    "        self.strategy = strategy\n",
    "        self.threshold = threshold\n",
    "        self.model = load_pkl(path)\n",
    "        self.features = features\n",
    "        \n",
    "    def predict(self, data_cp):\n",
    "        data = data_cp.copy()\n",
    "\n",
    "        data['model'] = self.name\n",
    "        \n",
    "        data['y_pred'] = self.model.predict(data[self.features])\n",
    "        data['y_pred_bin'] = (data['y_pred'].values >= self.threshold).astype(int)\n",
    "    \n",
    "        #можно для больших чисел без 6 знаков после запятой сделать\n",
    "        data['fix_lose'] = np.round(data['close'] * self.strategy[0], 6)\n",
    "        data['fix_win'] = np.round(data['close'] * self.strategy[1], 6)\n",
    "    \n",
    "        data = data[['time', 'ticker', 'close', 'model', 'y_pred', 'y_pred_bin', 'fix_lose', 'fix_win']]\n",
    "\n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51cd1416-878a-41a7-bef1-a4e1e56619f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL1 = lgbm_model(name='long_-2to+4_2days', \n",
    "                    strategy=(0.98, 1.04), \n",
    "                    threshold=0.3668169577019395, #99\n",
    "                    path='../bst_mdl/long_-2to+4_2days/model.pkl', \n",
    "                    features=MODEL_FEATURES1)\n",
    "\n",
    "\n",
    "MODEL2 = lgbm_model(name='short_+0.5to-1_1day', \n",
    "                    strategy=(1.005, 0.99), \n",
    "                    threshold=0.5695387939626755, #95\n",
    "                    path='../bst_mdl/short_+0.5to-1_1day/model.pkl', \n",
    "                    features=MODEL_FEATURES2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e1b82-84ce-4f46-b3b3-7012b96ba9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccff21e-6986-482e-a056-cb6ccd0e3c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81305c8f-4075-4d67-92e9-32759c5c70b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8283d53-db4b-4c69-a81a-45957adeac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lgbm(stocks):\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start lgbm predicting' +'\\n')\n",
    "\n",
    "    try:\n",
    "        hour_name = str(pd.Timestamp.now()).split(':')[0].replace(' ', '_')\n",
    "        data = load_pkl(f\"./data/lgbm/data_lgbm_{hour_name}.pkl\")\n",
    "\n",
    "        models = [MODEL1, MODEL2]\n",
    "\n",
    "        data_result = []\n",
    "        for model in models:\n",
    "            data_result += [model.predict(data)]\n",
    "            \n",
    "        data_result = pd.concat(data_result).reset_index(drop=True)\n",
    "        \n",
    "        dump_pkl(data_result, f\"./data/result/data_result_{hour_name}.pkl\")\n",
    "    except Exception as exception:\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' ' + f'Failed lgbm predicting ({exception})' +'\\n')\n",
    "\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End lgbm predicting' +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b08533f-3e3e-43e9-82d2-ed8bb732e9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988c862-8cd4-4d33-afc5-667dbb71a9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81640d3f-df60-4599-a6df-40ca85fdc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_lgbm(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db70a3dd-9809-4397-9558-f6df5460eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_pkl('./data/result/data_result_2025-02-04_16.pkl')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab230a73-ab5f-4339-9c80-afef1731e33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc5af7e0-8e00-42f7-9125-8bb21b5e59c5",
   "metadata": {},
   "source": [
    "# 7. Notification in telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8a86a89-7123-49c2-8b47-c84e62e7ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def escape_markdown(text):\n",
    "    \"\"\"функция для экранирования символов перед отправкой в маркдауне телеграма\"\"\"\n",
    "    pattern = r\"([_*\\[\\]()~|`])\"\n",
    "    return re.sub(pattern, r\"\\\\\\1\", text)\n",
    "\n",
    "def telegram_bot_sendtext(bot_message):\n",
    "    bot_message = escape_markdown(bot_message)\n",
    "    send_text = 'https://api.telegram.org/bot' + BOT_TOKEN + '/sendMessage?chat_id=' + BOT_CHAT_ID + '&parse_mode=Markdown&text=' + bot_message\n",
    "    response = requests.get(send_text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "# test = telegram_bot_sendtext(\"Testing Telegram bot\")\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e468c1b-51f2-44dd-af88-cf48ed5828d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40f36e09-2009-4209-a177-1b1b7ab62970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_pkl('./data/result/data_result_2025-02-04_16.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cceb79aa-d65c-46cf-9a81-d433c74beb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_notification_tg(stocks):    \n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start sending msg' +'\\n')\n",
    "    \n",
    "    try:\n",
    "        hour_name = str(pd.Timestamp.now()).split(':')[0].replace(' ', '_')\n",
    "        data = load_pkl(f\"./data/result/data_result_{hour_name}.pkl\")\n",
    "\n",
    "        msg = f'signals: {data['y_pred_bin'].sum()}\\n'\n",
    "\n",
    "        for model in data['model'].unique():\n",
    "            msg += f\"\\n{model}\\n\\n\"\n",
    "            data_model = data[data['model'] == model]\n",
    "            for i, row in data_model.iterrows():\n",
    "                if row['y_pred_bin'] == 1:\n",
    "                    msg += f\"{row['ticker']}, close = {row['close']}\\n\"\n",
    "                    sign = '<' if row['fix_lose'] < row['fix_win'] else '>'\n",
    "                    msg += f\"{row['fix_lose']} {sign} {row['fix_win']}\\n\\n\"\n",
    "\n",
    "        msg += '\\nno data:\\n'\n",
    "        no_data_stocks = set(stocks).difference(set(data['ticker'].tolist()))\n",
    "        msg += \"\\n\".join(list(no_data_stocks))\n",
    "        \n",
    "        _ = telegram_bot_sendtext(msg)\n",
    "              \n",
    "    except Exception as exception:\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' ' + f'Failed sending msg ({exception})' +'\\n')\n",
    "\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End sending msg' +'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47cacd-0877-4223-b74c-6f223b23e356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57414084-3fb6-46d6-a226-ab35bcc70b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send_notification_tg(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057405ed-1a19-497a-9c74-bcfe534fb8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2eace-a352-4976-8108-c3532520eab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "066f7791-9f74-451d-88b9-21826228eec2",
   "metadata": {},
   "source": [
    "# x. Daemon Khibiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ebc500b-b39c-4c1f-9a97-97b2c14ec509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_business_day(date):\n",
    "    return bool(len(pd.bdate_range(date, date)))\n",
    "\n",
    "def flag_work_1day(date):\n",
    "    time_name = str(pd.Timestamp.now().date()) \n",
    "    if (date.hour >= 10) and (date.minute >= 1) and is_business_day(date):\n",
    "        if (not os.path.exists(f'./data/raw/1day/{time_name}')):# if deamon is starting in the middle of the day\n",
    "            return True\n",
    "        if len(os.listdir(f'./data/raw/1day/{time_name}')) == 0: #если произошла ошибка при первой попытке скачивания данных\n",
    "            return True\n",
    "\n",
    "    if (date.hour == 10) and (date.minute == 50) and is_business_day(date): #дополнительное скачивнаие данных за пред дни для низколиквидных акций\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "    \n",
    "def flag_work_1hour(date):\n",
    "    if date.hour == 10 and date.minute == 3 and is_business_day(date): #отдельный случай, чтобы успели скачаться дневные данные (для проверки актуальности данных)\n",
    "        return True\n",
    "    if date.hour in range(11, 24) and date.minute == 1 and is_business_day(date):\n",
    "        return True\n",
    "    return False   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22eed5-d5ed-464e-9b47-3d7226771270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0daa38-54bf-45b7-b0f6-8c619f61c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-14 12:26:11\n"
     ]
    }
   ],
   "source": [
    "while 1 > 0:\n",
    "    clear_output()\n",
    "    cur_time = pd.Timestamp.now().floor('s')\n",
    "    print(cur_time)\n",
    "    \n",
    "    if flag_work_1day(cur_time):\n",
    "        get_data_1day(STOCKS)\n",
    "        preproc_data_1day(STOCKS)\n",
    "        \n",
    "    if flag_work_1hour(cur_time):   \n",
    "        get_data_1hour(STOCKS)\n",
    "        preproc_data_1hour(STOCKS)\n",
    "        prepare_data_lgbm(STOCKS, FEATURES)\n",
    "        predict_lgbm(STOCKS)\n",
    "        send_notification_tg(STOCKS)\n",
    "\n",
    "\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167b45a-038c-47a9-985c-6e61bf2be0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # get_data_1hour(STOCKS)\n",
    "        # preproc_data_1hour(STOCKS)\n",
    "        # prepare_data_lgbm(STOCKS, FEATURES)\n",
    "        # predict_lgbm(STOCKS)\n",
    "        # send_notification_tg(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc68b63-21d1-478b-81f4-f40149b0ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data_1hour(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb36ddec-ea51-4147-9892-4c2c9c891ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preproc_data_1hour(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363737d-4b6a-4b00-b1e4-205977aed315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data_1hour(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d649d8f-c678-420c-8f4f-c3d0f1538a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_pkl('./data/result/data_result_2025-02-10_15.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965393a-fa37-4268-80bc-0f0518b6e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y_pred'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e77c3f-8a53-49ec-ba0e-141dfb82ded6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdc49a-86f2-461a-8e11-255655c35090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d81767-6cdf-4118-acf2-5c1f6c5619dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
