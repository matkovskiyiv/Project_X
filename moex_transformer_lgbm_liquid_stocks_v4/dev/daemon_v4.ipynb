{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10843558-3e75-4b0a-807c-166b1370b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tinkoff.invest import Client, SecurityTradingStatus, CandleInterval\n",
    "from tinkoff.invest.services import InstrumentsService\n",
    "from tinkoff.invest.utils import quotation_to_decimal, now\n",
    "\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "\n",
    "import pickle\n",
    "def dump_pkl(data, filename):\n",
    "  with open(filename, 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "def load_pkl(filename):\n",
    "  with open(filename, 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "  return data\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d393b94-5d31-4aa1-b85b-0cb2da211f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41408bd-162d-4f94-896e-eabcb234ead4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f60bc2fd-d8fc-44c0-914e-60f8d4dd50a5",
   "metadata": {},
   "source": [
    "# 1. Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1430f0a3-bbe8-4d35-b6d7-7226331d5568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GetAccountsResponse(accounts=[Account(id='2169433725', type=<AccountType.ACCOUNT_TYPE_TINKOFF: 1>, name='M13', status=<AccountStatus.ACCOUNT_STATUS_OPEN: 2>, opened_date=datetime.datetime(2022, 4, 27, 0, 0, tzinfo=datetime.timezone.utc), closed_date=datetime.datetime(1970, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), access_level=<AccessLevel.ACCOUNT_ACCESS_LEVEL_READ_ONLY: 2>)])\n"
     ]
    }
   ],
   "source": [
    "TOKEN = 'xxx'\n",
    "with Client(TOKEN) as client:\n",
    "    print(client.users.get_accounts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5eba080-f5ae-43a2-a632-69578a1bf7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOT_TOKEN = '7598372117:AAHKSc-jHo02zSzJ-NOoZ8GloOqdbNySeZw'\n",
    "BOT_CHAT_ID = '131510115'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2393dbe4-6fe6-4c5d-9b10-b71502085b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FEATURES1 = ['close/close_w5_ma',\n",
    " 'close/close_w10_ma',\n",
    " 'close/close_w20_ma',\n",
    " 'close/close_w5_expma',\n",
    " 'close/close_w10_expma',\n",
    " 'close_w5_min/close']\n",
    "\n",
    "\n",
    "MODEL_FEATURES2 = ['close/close_w10_ma',\n",
    " 'close/close_w30_ma',\n",
    " 'close/close_w10_expma',\n",
    " 'close/close_w30_expma',\n",
    " 'close_w10_max/close',\n",
    " 'close/close_1hour_w10_expma']\n",
    "\n",
    "MODEL_FEATURES3 = ['close/close_w10_ma',\n",
    " 'close/close_w20_ma',\n",
    " 'close/close_w30_ma',\n",
    " 'close/close_w10_expma',\n",
    " 'close/close_w20_expma',\n",
    " 'close/close_w30_expma']\n",
    "\n",
    "FEATURES = list(set(MODEL_FEATURES1 + MODEL_FEATURES2 + MODEL_FEATURES3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "061da235-0767-4c15-a6ef-e26b6148b632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f40e85e-24ba-4eba-9cc8-d49996066b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbffffcb-a7bf-4795-82ad-00e0a4cda519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COUNT_POINTS = 120\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "STOCKS = ['AFLT',\n",
    " 'IRAO',\n",
    " 'MVID',\n",
    " 'SPBE',\n",
    " 'MTLRP',\n",
    " 'SFIN',\n",
    " 'FEES',\n",
    " 'LKOH',\n",
    " 'FESH',\n",
    " 'MDMG',\n",
    " 'VKCO',\n",
    " 'UPRO',\n",
    " 'SELG',\n",
    " 'SGZH',\n",
    " 'ENPG',\n",
    " 'LSRG',\n",
    " 'X5',\n",
    " 'NMTP',\n",
    " 'PLZL',\n",
    " 'TATN',\n",
    " 'AFKS',\n",
    " 'GMKN',\n",
    " 'RNFT',\n",
    " 'YDEX',\n",
    " 'UWGN',\n",
    " 'TGKN',\n",
    " 'HYDR',\n",
    " 'ABIO',\n",
    " 'WUSH',\n",
    " 'GTRK',\n",
    " 'NLMK',\n",
    " 'TATNP',\n",
    " 'ALRS',\n",
    " 'VSMO',\n",
    " 'TRNFP',\n",
    " 'MGNT',\n",
    " 'IRKT',\n",
    " 'MTLR',\n",
    " 'HEAD',\n",
    " 'RUAL',\n",
    " 'SIBN',\n",
    " 'SNGS',\n",
    " 'NVTK',\n",
    " 'RASP',\n",
    " 'VTBR',\n",
    " 'PHOR',\n",
    " 'T',\n",
    " 'TRMK',\n",
    " 'MOEX',\n",
    " 'SBERP',\n",
    " 'CHMF',\n",
    " 'RTKM',\n",
    " 'SMLT',\n",
    " 'UNAC',\n",
    " 'GAZP',\n",
    " 'ROSN',\n",
    " 'BELU',\n",
    " 'FLOT',\n",
    " 'PIKK',\n",
    " 'SVAV',\n",
    " 'POSI',\n",
    " 'SNGSP',\n",
    " 'MTSS',\n",
    " 'MAGN',\n",
    " 'SBER',\n",
    " 'BANEP',\n",
    " 'BSPB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6767d8-501e-470b-a0cf-d2b5bb6274db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f87bd0-980d-4f69-90f9-5c136771b23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4cb446d-836d-4f47-9926-29168770ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPS_15MIN = [\n",
    "        #windows: \n",
    "        #close: w1, w5, w10, w20, w30, w60, w120\n",
    "        #volume: w1, w5, w10, w20\n",
    "        #tmos_close: w1, w5, w10, w20, w30, w60, w120\n",
    "         ['close_w1_roc', 'close_w5_alpha', 'close_w10_alpha', 'close_w20_alpha', 'close_w30_alpha', 'close_w60_alpha', 'close_w120_alpha'],\n",
    "         ['volume_w1_roc', 'volume_w5_alpha', 'volume_w10_alpha', 'volume_w20_alpha'],\n",
    "         ['tmos_close_w1_roc', 'tmos_close_w5_alpha', 'tmos_close_w10_alpha', 'tmos_close_w20_alpha', 'tmos_close_w30_alpha', 'tmos_close_w60_alpha', 'tmos_close_w120_alpha'],\n",
    "    \n",
    "    \n",
    "         ['close_w1_roc', 'close_w5_roc', 'close_w10_roc', 'close_w20_roc', 'close_w30_roc', 'close_w60_roc', 'close_w120_roc'],\n",
    "         ['volume_w1_roc', 'volume_w5_roc', 'volume_w10_roc', 'volume_w20_roc'],\n",
    "         ['tmos_close_w1_roc', 'tmos_close_w5_roc', 'tmos_close_w10_roc', 'tmos_close_w20_roc', 'tmos_close_w30_roc', 'tmos_close_w60_roc', 'tmos_close_w120_roc'],\n",
    "\n",
    "    \n",
    "         ['close_w5_mean_abs_pct', 'close_w10_mean_abs_pct', 'close_w20_mean_abs_pct', 'close_w30_mean_abs_pct', 'close_w60_mean_abs_pct', 'close_w120_mean_abs_pct'],\n",
    "         ['volume_w5_mean_abs_pct', 'volume_w10_mean_abs_pct', 'volume_w20_mean_abs_pct'],\n",
    "         ['tmos_close_w5_mean_abs_pct', 'tmos_close_w10_mean_abs_pct', 'tmos_close_w20_mean_abs_pct', 'tmos_close_w30_mean_abs_pct', 'tmos_close_w60_mean_abs_pct', 'tmos_close_w120_mean_abs_pct'],\n",
    "\n",
    "\n",
    "         ['close_w5_std', 'close_w10_std', 'close_w20_std', 'close_w30_std', 'close_w60_std', 'close_w120_std'],\n",
    "         ['volume_w5_std', 'volume_w10_std', 'volume_w20_std'],\n",
    "         ['tmos_close_w5_std', 'tmos_close_w10_std', 'tmos_close_w20_std', 'tmos_close_w30_std', 'tmos_close_w60_std', 'tmos_close_w120_std'],\n",
    "\n",
    "\n",
    "         ['close_w5_norm_std', 'close_w10_norm_std', 'close_w20_norm_std', 'close_w30_norm_std', 'close_w60_norm_std', 'close_w120_norm_std'],\n",
    "         ['volume_w5_norm_std', 'volume_w10_norm_std', 'volume_w20_norm_std'],\n",
    "         ['tmos_close_w5_norm_std', 'tmos_close_w10_norm_std', 'tmos_close_w20_norm_std', 'tmos_close_w30_norm_std', 'tmos_close_w60_norm_std', 'tmos_close_w120_norm_std'],\n",
    "\n",
    "\n",
    "         ['close_w5_rsi', 'close_w10_rsi', 'close_w20_rsi', 'close_w30_rsi', 'close_w60_rsi', 'close_w120_rsi'],\n",
    "         ['volume_w5_rsi', 'volume_w10_rsi', 'volume_w20_rsi'],\n",
    "         ['tmos_close_w5_rsi', 'tmos_close_w10_rsi', 'tmos_close_w20_rsi', 'tmos_close_w30_rsi', 'tmos_close_w60_rsi', 'tmos_close_w120_rsi'],\n",
    "\n",
    "\n",
    "         ['close', 'close_w5_ma', 'close_w10_ma', 'close_w20_ma', 'close_w30_ma', 'close_w60_ma', 'close_w120_ma'],\n",
    "         ['volume', 'volume_w5_ma', 'volume_w10_ma', 'volume_w20_ma'],\n",
    "         ['tmos_close', 'tmos_close_w5_ma', 'tmos_close_w10_ma', 'tmos_close_w20_ma', 'tmos_close_w30_ma', 'tmos_close_w60_ma', 'tmos_close_w120_ma'],\n",
    "\n",
    "\n",
    "         ['close', 'close_w5_expma', 'close_w10_expma', 'close_w20_expma', 'close_w30_expma', 'close_w60_expma', 'close_w120_expma'],\n",
    "         ['volume', 'volume_w5_expma', 'volume_w10_expma', 'volume_w20_expma'],\n",
    "         ['tmos_close', 'tmos_close_w5_expma', 'tmos_close_w10_expma', 'tmos_close_w20_expma', 'tmos_close_w30_expma', 'tmos_close_w60_expma', 'tmos_close_w120_expma'],\n",
    "\n",
    "\n",
    "         {'close' : ['close_w5_min', 'close_w10_min', 'close_w20_min', 'close_w30_min', 'close_w60_min', 'close_w120_min']},\n",
    "         {'volume' : ['volume_w5_min', 'volume_w10_min', 'volume_w20_min']},\n",
    "         {'tmos_close' : ['tmos_close_w5_min', 'tmos_close_w10_min', 'tmos_close_w20_min', 'tmos_close_w30_min', 'tmos_close_w60_min', 'tmos_close_w120_min']},\n",
    "\n",
    "    \n",
    "         {'close' : ['close_w5_max', 'close_w10_max', 'close_w20_max', 'close_w30_max', 'close_w60_max', 'close_w120_max']},\n",
    "         {'volume' : ['volume_w5_max', 'volume_w10_max', 'volume_w20_max']},\n",
    "         {'tmos_close' : ['tmos_close_w5_max', 'tmos_close_w10_max', 'tmos_close_w20_max', 'tmos_close_w30_max', 'tmos_close_w60_max', 'tmos_close_w120_max']},\n",
    "\n",
    "\n",
    "\n",
    "        #w5\n",
    "         {'close' : [  'close_w5_ma_low_2std', 'close_w5_ma_up_2std', 'close_w5_ma_low_3std', 'close_w5_ma_up_3std']},\n",
    "         {'volume' : [  'volume_w5_ma_low_2std', 'volume_w5_ma_up_2std', 'volume_w5_ma_low_3std', 'volume_w5_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_w5_ma_low_2std', 'tmos_close_w5_ma_up_2std', 'tmos_close_w5_ma_low_3std', 'tmos_close_w5_ma_up_3std']},\n",
    "        #w10\n",
    "         {'close' : [  'close_w10_ma_low_2std', 'close_w10_ma_up_2std', 'close_w10_ma_low_3std', 'close_w10_ma_up_3std']},\n",
    "         {'volume' : [  'volume_w10_ma_low_2std', 'volume_w10_ma_up_2std', 'volume_w10_ma_low_3std', 'volume_w10_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_w10_ma_low_2std', 'tmos_close_w10_ma_up_2std', 'tmos_close_w10_ma_low_3std', 'tmos_close_w10_ma_up_3std']},\n",
    "        #w20\n",
    "         {'close' : [  'close_w20_ma_low_2std', 'close_w20_ma_up_2std', 'close_w20_ma_low_3std', 'close_w20_ma_up_3std']},\n",
    "         {'volume' : [  'volume_w20_ma_low_2std', 'volume_w20_ma_up_2std', 'volume_w20_ma_low_3std', 'volume_w20_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_w20_ma_low_2std', 'tmos_close_w20_ma_up_2std', 'tmos_close_w20_ma_low_3std', 'tmos_close_w20_ma_up_3std']},\n",
    "        #w30\n",
    "         {'close' : [  'close_w30_ma_low_2std', 'close_w30_ma_up_2std', 'close_w30_ma_low_3std', 'close_w30_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_w30_ma_low_2std', 'tmos_close_w30_ma_up_2std', 'tmos_close_w30_ma_low_3std', 'tmos_close_w30_ma_up_3std']},\n",
    "        #w60\n",
    "         {'close' : [  'close_w60_ma_low_2std', 'close_w60_ma_up_2std', 'close_w60_ma_low_3std', 'close_w60_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_w60_ma_low_2std', 'tmos_close_w60_ma_up_2std', 'tmos_close_w60_ma_low_3std', 'tmos_close_w60_ma_up_3std']},\n",
    "        #w120\n",
    "         {'close' : [  'close_w120_ma_low_2std', 'close_w120_ma_up_2std', 'close_w120_ma_low_3std', 'close_w120_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_w120_ma_low_2std', 'tmos_close_w120_ma_up_2std', 'tmos_close_w120_ma_low_3std', 'tmos_close_w120_ma_up_3std']},\n",
    "\n",
    "\n",
    "        #w5\n",
    "        ['close_w5_min', 'close_w5_max'],\n",
    "        ['volume_w5_min', 'volume_w5_max'],\n",
    "        ['tmos_close_w5_min', 'tmos_close_w5_max'],\n",
    "        #w10\n",
    "        ['close_w10_min', 'close_w10_max'],\n",
    "        ['volume_w10_min', 'volume_w10_max'],\n",
    "        ['tmos_close_w10_min', 'tmos_close_w10_max'],\n",
    "        #w20\n",
    "        ['close_w20_min', 'close_w20_max'],\n",
    "        ['volume_w20_min', 'volume_w20_max'],\n",
    "        ['tmos_close_w20_min', 'tmos_close_w20_max'],\n",
    "        #w30\n",
    "        ['close_w30_min', 'close_w30_max'],\n",
    "        ['tmos_close_w30_min', 'tmos_close_w30_max'],\n",
    "        #w60\n",
    "        ['close_w60_min', 'close_w60_max'],\n",
    "        ['tmos_close_w60_min', 'tmos_close_w60_max'],\n",
    "        #w120\n",
    "        ['close_w120_min', 'close_w120_max'],\n",
    "        ['tmos_close_w120_min', 'tmos_close_w120_max'],\n",
    "]\n",
    "\n",
    "\n",
    "GROUPS_1HOUR = [\n",
    "        #windows: \n",
    "        #close: w1, w5, w10, w20, w30, w60, w120\n",
    "        #volume: w1, w5, w10, w20\n",
    "        #tmos_close: w1, w5, w10, w20, w30, w60, w120\n",
    "         ['close_1hour_w1_roc', 'close_1hour_w5_alpha', 'close_1hour_w10_alpha', 'close_1hour_w20_alpha', 'close_1hour_w30_alpha', 'close_1hour_w60_alpha', 'close_1hour_w120_alpha'],\n",
    "         ['volume_1hour_w1_roc', 'volume_1hour_w5_alpha', 'volume_1hour_w10_alpha', 'volume_1hour_w20_alpha'],\n",
    "         ['tmos_close_1hour_w1_roc', 'tmos_close_1hour_w5_alpha', 'tmos_close_1hour_w10_alpha', 'tmos_close_1hour_w20_alpha', 'tmos_close_1hour_w30_alpha', 'tmos_close_1hour_w60_alpha', 'tmos_close_1hour_w120_alpha'],\n",
    "    \n",
    "         ['close_1hour_w1_roc', 'close_1hour_w5_roc', 'close_1hour_w10_roc', 'close_1hour_w20_roc', 'close_1hour_w30_roc', 'close_1hour_w60_roc', 'close_1hour_w120_roc'],\n",
    "         ['volume_1hour_w1_roc', 'volume_1hour_w5_roc', 'volume_1hour_w10_roc', 'volume_1hour_w20_roc'],\n",
    "         ['tmos_close_1hour_w1_roc', 'tmos_close_1hour_w5_roc', 'tmos_close_1hour_w10_roc', 'tmos_close_1hour_w20_roc', 'tmos_close_1hour_w30_roc', 'tmos_close_1hour_w60_roc', 'tmos_close_1hour_w120_roc'],\n",
    "\n",
    "         ['close_1hour_w5_mean_abs_pct', 'close_1hour_w10_mean_abs_pct', 'close_1hour_w20_mean_abs_pct', 'close_1hour_w30_mean_abs_pct', 'close_1hour_w60_mean_abs_pct', 'close_1hour_w120_mean_abs_pct'],\n",
    "         ['volume_1hour_w5_mean_abs_pct', 'volume_1hour_w10_mean_abs_pct', 'volume_1hour_w20_mean_abs_pct'],\n",
    "         ['tmos_close_1hour_w5_mean_abs_pct', 'tmos_close_1hour_w10_mean_abs_pct', 'tmos_close_1hour_w20_mean_abs_pct', 'tmos_close_1hour_w30_mean_abs_pct', 'tmos_close_1hour_w60_mean_abs_pct', 'tmos_close_1hour_w120_mean_abs_pct'],\n",
    "\n",
    "\n",
    "         ['close_1hour_w5_std', 'close_1hour_w10_std', 'close_1hour_w20_std', 'close_1hour_w30_std', 'close_1hour_w60_std', 'close_1hour_w120_std'],\n",
    "         ['volume_1hour_w5_std', 'volume_1hour_w10_std', 'volume_1hour_w20_std'],\n",
    "         ['tmos_close_1hour_w5_std', 'tmos_close_1hour_w10_std', 'tmos_close_1hour_w20_std', 'tmos_close_1hour_w30_std', 'tmos_close_1hour_w60_std', 'tmos_close_1hour_w120_std'],\n",
    "\n",
    "\n",
    "         ['close_1hour_w5_norm_std', 'close_1hour_w10_norm_std', 'close_1hour_w20_norm_std', 'close_1hour_w30_norm_std', 'close_1hour_w60_norm_std', 'close_1hour_w120_norm_std'],\n",
    "         ['volume_1hour_w5_norm_std', 'volume_1hour_w10_norm_std', 'volume_1hour_w20_norm_std'],\n",
    "         ['tmos_close_1hour_w5_norm_std', 'tmos_close_1hour_w10_norm_std', 'tmos_close_1hour_w20_norm_std', 'tmos_close_1hour_w30_norm_std', 'tmos_close_1hour_w60_norm_std', 'tmos_close_1hour_w120_norm_std'],\n",
    "\n",
    "\n",
    "         ['close_1hour_w5_rsi', 'close_1hour_w10_rsi', 'close_1hour_w20_rsi', 'close_1hour_w30_rsi', 'close_1hour_w60_rsi', 'close_1hour_w120_rsi'],\n",
    "         ['volume_1hour_w5_rsi', 'volume_1hour_w10_rsi', 'volume_1hour_w20_rsi'],\n",
    "         ['tmos_close_1hour_w5_rsi', 'tmos_close_1hour_w10_rsi', 'tmos_close_1hour_w20_rsi', 'tmos_close_1hour_w30_rsi', 'tmos_close_1hour_w60_rsi', 'tmos_close_1hour_w120_rsi'],\n",
    "\n",
    "\n",
    "         ['close', 'close_1hour_w5_ma', 'close_1hour_w10_ma', 'close_1hour_w20_ma', 'close_1hour_w30_ma', 'close_1hour_w60_ma', 'close_1hour_w120_ma'],\n",
    "         ['volume', 'volume_1hour_w5_ma', 'volume_1hour_w10_ma', 'volume_1hour_w20_ma'],\n",
    "         ['tmos_close', 'tmos_close_1hour_w5_ma', 'tmos_close_1hour_w10_ma', 'tmos_close_1hour_w20_ma', 'tmos_close_1hour_w30_ma', 'tmos_close_1hour_w60_ma', 'tmos_close_1hour_w120_ma'],\n",
    "\n",
    "\n",
    "         ['close', 'close_1hour_w5_expma', 'close_1hour_w10_expma', 'close_1hour_w20_expma', 'close_1hour_w30_expma', 'close_1hour_w60_expma', 'close_1hour_w120_expma'],\n",
    "         ['volume', 'volume_1hour_w5_expma', 'volume_1hour_w10_expma', 'volume_1hour_w20_expma'],\n",
    "         ['tmos_close', 'tmos_close_1hour_w5_expma', 'tmos_close_1hour_w10_expma', 'tmos_close_1hour_w20_expma', 'tmos_close_1hour_w30_expma', 'tmos_close_1hour_w60_expma', 'tmos_close_1hour_w120_expma'],\n",
    "\n",
    "\n",
    "         {'close' : ['close_1hour_w5_min', 'close_1hour_w10_min', 'close_1hour_w20_min', 'close_1hour_w30_min', 'close_1hour_w60_min', 'close_1hour_w120_min']},\n",
    "         {'volume' : ['volume_1hour_w5_min', 'volume_1hour_w10_min', 'volume_1hour_w20_min']},\n",
    "         {'tmos_close' : ['tmos_close_1hour_w5_min', 'tmos_close_1hour_w10_min', 'tmos_close_1hour_w20_min', 'tmos_close_1hour_w30_min', 'tmos_close_1hour_w60_min', 'tmos_close_1hour_w120_min']},\n",
    "\n",
    "    \n",
    "         {'close' : ['close_1hour_w5_max', 'close_1hour_w10_max', 'close_1hour_w20_max', 'close_1hour_w30_max', 'close_1hour_w60_max', 'close_1hour_w120_max']},\n",
    "         {'volume' : ['volume_1hour_w5_max', 'volume_1hour_w10_max', 'volume_1hour_w20_max']},\n",
    "         {'tmos_close' : ['tmos_close_1hour_w5_max', 'tmos_close_1hour_w10_max', 'tmos_close_1hour_w20_max', 'tmos_close_1hour_w30_max', 'tmos_close_1hour_w60_max', 'tmos_close_1hour_w120_max']},\n",
    "\n",
    "\n",
    "\n",
    "        #w5\n",
    "         {'close' : [  'close_1hour_w5_ma_low_2std', 'close_1hour_w5_ma_up_2std', 'close_1hour_w5_ma_low_3std', 'close_1hour_w5_ma_up_3std']},\n",
    "         {'volume' : [  'volume_1hour_w5_ma_low_2std', 'volume_1hour_w5_ma_up_2std', 'volume_1hour_w5_ma_low_3std', 'volume_1hour_w5_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_1hour_w5_ma_low_2std', 'tmos_close_1hour_w5_ma_up_2std', 'tmos_close_1hour_w5_ma_low_3std', 'tmos_close_1hour_w5_ma_up_3std']},\n",
    "        #w10\n",
    "         {'close' : [  'close_1hour_w10_ma_low_2std', 'close_1hour_w10_ma_up_2std', 'close_1hour_w10_ma_low_3std', 'close_1hour_w10_ma_up_3std']},\n",
    "         {'volume' : [  'volume_1hour_w10_ma_low_2std', 'volume_1hour_w10_ma_up_2std', 'volume_1hour_w10_ma_low_3std', 'volume_1hour_w10_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_1hour_w10_ma_low_2std', 'tmos_close_1hour_w10_ma_up_2std', 'tmos_close_1hour_w10_ma_low_3std', 'tmos_close_1hour_w10_ma_up_3std']},\n",
    "        #w20\n",
    "         {'close' : [  'close_1hour_w20_ma_low_2std', 'close_1hour_w20_ma_up_2std', 'close_1hour_w20_ma_low_3std', 'close_1hour_w20_ma_up_3std']},\n",
    "         {'volume' : [  'volume_1hour_w20_ma_low_2std', 'volume_1hour_w20_ma_up_2std', 'volume_1hour_w20_ma_low_3std', 'volume_1hour_w20_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_1hour_w20_ma_low_2std', 'tmos_close_1hour_w20_ma_up_2std', 'tmos_close_1hour_w20_ma_low_3std', 'tmos_close_1hour_w20_ma_up_3std']},\n",
    "        #w30\n",
    "         {'close' : [  'close_1hour_w30_ma_low_2std', 'close_1hour_w30_ma_up_2std', 'close_1hour_w30_ma_low_3std', 'close_1hour_w30_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_1hour_w30_ma_low_2std', 'tmos_close_1hour_w30_ma_up_2std', 'tmos_close_1hour_w30_ma_low_3std', 'tmos_close_1hour_w30_ma_up_3std']},\n",
    "        #w60\n",
    "         {'close' : [  'close_1hour_w60_ma_low_2std', 'close_1hour_w60_ma_up_2std', 'close_1hour_w60_ma_low_3std', 'close_1hour_w60_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_1hour_w60_ma_low_2std', 'tmos_close_1hour_w60_ma_up_2std', 'tmos_close_1hour_w60_ma_low_3std', 'tmos_close_1hour_w60_ma_up_3std']},\n",
    "        #w120\n",
    "         {'close' : [  'close_1hour_w120_ma_low_2std', 'close_1hour_w120_ma_up_2std', 'close_1hour_w120_ma_low_3std', 'close_1hour_w120_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_1hour_w120_ma_low_2std', 'tmos_close_1hour_w120_ma_up_2std', 'tmos_close_1hour_w120_ma_low_3std', 'tmos_close_1hour_w120_ma_up_3std']},\n",
    "\n",
    "\n",
    "        #w5\n",
    "        ['close_1hour_w5_min', 'close_1hour_w5_max'],\n",
    "        ['volume_1hour_w5_min', 'volume_1hour_w5_max'],\n",
    "        ['tmos_close_1hour_w5_min', 'tmos_close_1hour_w5_max'],\n",
    "        #w10\n",
    "        ['close_1hour_w10_min', 'close_1hour_w10_max'],\n",
    "        ['volume_1hour_w10_min', 'volume_1hour_w10_max'],\n",
    "        ['tmos_close_1hour_w10_min', 'tmos_close_1hour_w10_max'],\n",
    "        #w20\n",
    "        ['close_1hour_w20_min', 'close_1hour_w20_max'],\n",
    "        ['volume_1hour_w20_min', 'volume_1hour_w20_max'],\n",
    "        ['tmos_close_1hour_w20_min', 'tmos_close_1hour_w20_max'],\n",
    "        #w30\n",
    "        ['close_1hour_w30_min', 'close_1hour_w30_max'],\n",
    "        ['tmos_close_1hour_w30_min', 'tmos_close_1hour_w30_max'],\n",
    "        #w60\n",
    "        ['close_1hour_w60_min', 'close_1hour_w60_max'],\n",
    "        ['tmos_close_1hour_w60_min', 'tmos_close_1hour_w60_max'],\n",
    "        #w120\n",
    "        ['close_1hour_w120_min', 'close_1hour_w120_max'],\n",
    "        ['tmos_close_1hour_w120_min', 'tmos_close_1hour_w120_max'],\n",
    "]\n",
    "\n",
    "\n",
    "GROUPS_1DAY= [\n",
    "        #windows: \n",
    "        #close: w1, w5, w10, w20, w30, w60, w120\n",
    "        #volume: w1, w5, w10, w20\n",
    "        #tmos_close: w1, w5, w10, w20, w30, w60, w120\n",
    "         ['close_1day_w1_roc', 'close_1day_w5_alpha', 'close_1day_w10_alpha', 'close_1day_w20_alpha', 'close_1day_w30_alpha', 'close_1day_w60_alpha', 'close_1day_w120_alpha'],\n",
    "         ['volume_1day_w1_roc', 'volume_1day_w5_alpha', 'volume_1day_w10_alpha', 'volume_1day_w20_alpha'],\n",
    "         ['tmos_close_1day_w1_roc', 'tmos_close_1day_w5_alpha', 'tmos_close_1day_w10_alpha', 'tmos_close_1day_w20_alpha', 'tmos_close_1day_w30_alpha', 'tmos_close_1day_w60_alpha', 'tmos_close_1day_w120_alpha'],\n",
    "    \n",
    "         ['close_1day_w1_roc', 'close_1day_w5_roc', 'close_1day_w10_roc', 'close_1day_w20_roc', 'close_1day_w30_roc', 'close_1day_w60_roc', 'close_1day_w120_roc'],\n",
    "         ['volume_1day_w1_roc', 'volume_1day_w5_roc', 'volume_1day_w10_roc', 'volume_1day_w20_roc'],\n",
    "         ['tmos_close_1day_w1_roc', 'tmos_close_1day_w5_roc', 'tmos_close_1day_w10_roc', 'tmos_close_1day_w20_roc', 'tmos_close_1day_w30_roc', 'tmos_close_1day_w60_roc', 'tmos_close_1day_w120_roc'],\n",
    "\n",
    "         ['close_1day_w5_mean_abs_pct', 'close_1day_w10_mean_abs_pct', 'close_1day_w20_mean_abs_pct', 'close_1day_w30_mean_abs_pct', 'close_1day_w60_mean_abs_pct', 'close_1day_w120_mean_abs_pct'],\n",
    "         ['volume_1day_w5_mean_abs_pct', 'volume_1day_w10_mean_abs_pct', 'volume_1day_w20_mean_abs_pct'],\n",
    "         ['tmos_close_1day_w5_mean_abs_pct', 'tmos_close_1day_w10_mean_abs_pct', 'tmos_close_1day_w20_mean_abs_pct', 'tmos_close_1day_w30_mean_abs_pct', 'tmos_close_1day_w60_mean_abs_pct', 'tmos_close_1day_w120_mean_abs_pct'],\n",
    "\n",
    "\n",
    "         ['close_1day_w5_std', 'close_1day_w10_std', 'close_1day_w20_std', 'close_1day_w30_std', 'close_1day_w60_std', 'close_1day_w120_std'],\n",
    "         ['volume_1day_w5_std', 'volume_1day_w10_std', 'volume_1day_w20_std'],\n",
    "         ['tmos_close_1day_w5_std', 'tmos_close_1day_w10_std', 'tmos_close_1day_w20_std', 'tmos_close_1day_w30_std', 'tmos_close_1day_w60_std', 'tmos_close_1day_w120_std'],\n",
    "\n",
    "\n",
    "         ['close_1day_w5_norm_std', 'close_1day_w10_norm_std', 'close_1day_w20_norm_std', 'close_1day_w30_norm_std', 'close_1day_w60_norm_std', 'close_1day_w120_norm_std'],\n",
    "         ['volume_1day_w5_norm_std', 'volume_1day_w10_norm_std', 'volume_1day_w20_norm_std'],\n",
    "         ['tmos_close_1day_w5_norm_std', 'tmos_close_1day_w10_norm_std', 'tmos_close_1day_w20_norm_std', 'tmos_close_1day_w30_norm_std', 'tmos_close_1day_w60_norm_std', 'tmos_close_1day_w120_norm_std'],\n",
    "\n",
    "\n",
    "         ['close_1day_w5_rsi', 'close_1day_w10_rsi', 'close_1day_w20_rsi', 'close_1day_w30_rsi', 'close_1day_w60_rsi', 'close_1day_w120_rsi'],\n",
    "         ['volume_1day_w5_rsi', 'volume_1day_w10_rsi', 'volume_1day_w20_rsi'],\n",
    "         ['tmos_close_1day_w5_rsi', 'tmos_close_1day_w10_rsi', 'tmos_close_1day_w20_rsi', 'tmos_close_1day_w30_rsi', 'tmos_close_1day_w60_rsi', 'tmos_close_1day_w120_rsi'],\n",
    "\n",
    "\n",
    "         ['close', 'close_1day_w5_ma', 'close_1day_w10_ma', 'close_1day_w20_ma', 'close_1day_w30_ma', 'close_1day_w60_ma', 'close_1day_w120_ma'],\n",
    "         ['volume', 'volume_1day_w5_ma', 'volume_1day_w10_ma', 'volume_1day_w20_ma'],\n",
    "         ['tmos_close', 'tmos_close_1day_w5_ma', 'tmos_close_1day_w10_ma', 'tmos_close_1day_w20_ma', 'tmos_close_1day_w30_ma', 'tmos_close_1day_w60_ma', 'tmos_close_1day_w120_ma'],\n",
    "\n",
    "\n",
    "         ['close', 'close_1day_w5_expma', 'close_1day_w10_expma', 'close_1day_w20_expma', 'close_1day_w30_expma', 'close_1day_w60_expma', 'close_1day_w120_expma'],\n",
    "         ['volume', 'volume_1day_w5_expma', 'volume_1day_w10_expma', 'volume_1day_w20_expma'],\n",
    "         ['tmos_close', 'tmos_close_1day_w5_expma', 'tmos_close_1day_w10_expma', 'tmos_close_1day_w20_expma', 'tmos_close_1day_w30_expma', 'tmos_close_1day_w60_expma', 'tmos_close_1day_w120_expma'],\n",
    "\n",
    "\n",
    "         {'close' : ['close_1day_w5_min', 'close_1day_w10_min', 'close_1day_w20_min', 'close_1day_w30_min', 'close_1day_w60_min', 'close_1day_w120_min']},\n",
    "         {'volume' : ['volume_1day_w5_min', 'volume_1day_w10_min', 'volume_1day_w20_min']},\n",
    "         {'tmos_close' : ['tmos_close_1day_w5_min', 'tmos_close_1day_w10_min', 'tmos_close_1day_w20_min', 'tmos_close_1day_w30_min', 'tmos_close_1day_w60_min', 'tmos_close_1day_w120_min']},\n",
    "\n",
    "    \n",
    "         {'close' : ['close_1day_w5_max', 'close_1day_w10_max', 'close_1day_w20_max', 'close_1day_w30_max', 'close_1day_w60_max', 'close_1day_w120_max']},\n",
    "         {'volume' : ['volume_1day_w5_max', 'volume_1day_w10_max', 'volume_1day_w20_max']},\n",
    "         {'tmos_close' : ['tmos_close_1day_w5_max', 'tmos_close_1day_w10_max', 'tmos_close_1day_w20_max', 'tmos_close_1day_w30_max', 'tmos_close_1day_w60_max', 'tmos_close_1day_w120_max']},\n",
    "\n",
    "\n",
    "\n",
    "        #w5\n",
    "         {'close' : [  'close_1day_w5_ma_low_2std', 'close_1day_w5_ma_up_2std', 'close_1day_w5_ma_low_3std', 'close_1day_w5_ma_up_3std']},\n",
    "         {'volume' : [  'volume_1day_w5_ma_low_2std', 'volume_1day_w5_ma_up_2std', 'volume_1day_w5_ma_low_3std', 'volume_1day_w5_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_1day_w5_ma_low_2std', 'tmos_close_1day_w5_ma_up_2std', 'tmos_close_1day_w5_ma_low_3std', 'tmos_close_1day_w5_ma_up_3std']},\n",
    "        #w10\n",
    "         {'close' : [  'close_1day_w10_ma_low_2std', 'close_1day_w10_ma_up_2std', 'close_1day_w10_ma_low_3std', 'close_1day_w10_ma_up_3std']},\n",
    "         {'volume' : [  'volume_1day_w10_ma_low_2std', 'volume_1day_w10_ma_up_2std', 'volume_1day_w10_ma_low_3std', 'volume_1day_w10_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_1day_w10_ma_low_2std', 'tmos_close_1day_w10_ma_up_2std', 'tmos_close_1day_w10_ma_low_3std', 'tmos_close_1day_w10_ma_up_3std']},\n",
    "        #w20\n",
    "         {'close' : [  'close_1day_w20_ma_low_2std', 'close_1day_w20_ma_up_2std', 'close_1day_w20_ma_low_3std', 'close_1day_w20_ma_up_3std']},\n",
    "         {'volume' : [  'volume_1day_w20_ma_low_2std', 'volume_1day_w20_ma_up_2std', 'volume_1day_w20_ma_low_3std', 'volume_1day_w20_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_1day_w20_ma_low_2std', 'tmos_close_1day_w20_ma_up_2std', 'tmos_close_1day_w20_ma_low_3std', 'tmos_close_1day_w20_ma_up_3std']},\n",
    "        #w30\n",
    "         {'close' : [  'close_1day_w30_ma_low_2std', 'close_1day_w30_ma_up_2std', 'close_1day_w30_ma_low_3std', 'close_1day_w30_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_1day_w30_ma_low_2std', 'tmos_close_1day_w30_ma_up_2std', 'tmos_close_1day_w30_ma_low_3std', 'tmos_close_1day_w30_ma_up_3std']},\n",
    "        #w60\n",
    "         {'close' : [  'close_1day_w60_ma_low_2std', 'close_1day_w60_ma_up_2std', 'close_1day_w60_ma_low_3std', 'close_1day_w60_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_1day_w60_ma_low_2std', 'tmos_close_1day_w60_ma_up_2std', 'tmos_close_1day_w60_ma_low_3std', 'tmos_close_1day_w60_ma_up_3std']},\n",
    "        #w120\n",
    "         {'close' : [  'close_1day_w120_ma_low_2std', 'close_1day_w120_ma_up_2std', 'close_1day_w120_ma_low_3std', 'close_1day_w120_ma_up_3std']},\n",
    "         {'tmos_close' : [  'tmos_close_1day_w120_ma_low_2std', 'tmos_close_1day_w120_ma_up_2std', 'tmos_close_1day_w120_ma_low_3std', 'tmos_close_1day_w120_ma_up_3std']},\n",
    "\n",
    "\n",
    "        #w5\n",
    "        ['close_1day_w5_min', 'close_1day_w5_max'],\n",
    "        ['volume_1day_w5_min', 'volume_1day_w5_max'],\n",
    "        ['tmos_close_1day_w5_min', 'tmos_close_1day_w5_max'],\n",
    "        #w10\n",
    "        ['close_1day_w10_min', 'close_1day_w10_max'],\n",
    "        ['volume_1day_w10_min', 'volume_1day_w10_max'],\n",
    "        ['tmos_close_1day_w10_min', 'tmos_close_1day_w10_max'],\n",
    "        #w20\n",
    "        ['close_1day_w20_min', 'close_1day_w20_max'],\n",
    "        ['volume_1day_w20_min', 'volume_1day_w20_max'],\n",
    "        ['tmos_close_1day_w20_min', 'tmos_close_1day_w20_max'],\n",
    "        #w30\n",
    "        ['close_1day_w30_min', 'close_1day_w30_max'],\n",
    "        ['tmos_close_1day_w30_min', 'tmos_close_1day_w30_max'],\n",
    "        #w60\n",
    "        ['close_1day_w60_min', 'close_1day_w60_max'],\n",
    "        ['tmos_close_1day_w60_min', 'tmos_close_1day_w60_max'],\n",
    "        #w120\n",
    "        ['close_1day_w120_min', 'close_1day_w120_max'],\n",
    "        ['tmos_close_1day_w120_min', 'tmos_close_1day_w120_max'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868d0b9-4acc-4efc-815d-b897b4ae5cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af67e237-6c67-413d-b554-62bc82eea6c6",
   "metadata": {},
   "source": [
    "# 2. Make dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db61d334-729c-499c-8c88-94699c9152c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c324a1bc-adc1-48bd-8d78-26a255c13733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/raw: File exists\n",
      "mkdir: data/preproc: File exists\n",
      "mkdir: data/lgbm: File exists\n",
      "mkdir: data/result: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/raw data/preproc data/lgbm data/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06aa25cd-e957-472e-b78a-06bf1ce4b53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/raw/15min: File exists\n",
      "mkdir: data/raw/1hour: File exists\n",
      "mkdir: data/raw/1day: File exists\n",
      "mkdir: data/preproc/15min: File exists\n",
      "mkdir: data/preproc/1hour: File exists\n",
      "mkdir: data/preproc/1day: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/raw/15min data/raw/1hour data/raw/1day    data/preproc/15min data/preproc/1hour data/preproc/1day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c9d3e-35d2-4c61-a061-db9d7da21b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6f18d-b4fc-430c-a2e9-16962ecfc2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f85319c2-7e5a-44cf-a237-e3c8cd2c5035",
   "metadata": {},
   "source": [
    "# 2. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b404f1ca-4197-4da0-ad8b-0fac8c925744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_logs(text):\n",
    "    with open(\"logs.txt\", \"a\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde59a8-b20d-416d-8559-e37c3cb3c667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3357b12-dfcc-473f-a6e2-e4e760960312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_figi():\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start loading figi data' +'\\n')\n",
    "    #make dir\n",
    "\n",
    "    count_tries = 0\n",
    "    max_count_tries = 5\n",
    "    flag_data = False\n",
    "    while (not flag_data) and (count_tries < max_count_tries):\n",
    "        count_tries += 1\n",
    "        try:\n",
    "            with Client(TOKEN) as client:\n",
    "                instruments: InstrumentsService = client.instruments\n",
    "                tickers = []\n",
    "                for method in [\"shares\", \"bonds\", \"etfs\", \"currencies\", \"futures\"]:\n",
    "                    for item in getattr(instruments, method)().instruments:\n",
    "                        tickers.append(\n",
    "                            {\n",
    "                                \"name\": item.name,\n",
    "                                \"ticker\": item.ticker,\n",
    "                                \"class_code\": item.class_code,\n",
    "                                \"figi\": item.figi,\n",
    "                                \"uid\": item.uid,\n",
    "                                \"type\": method,\n",
    "                                \"min_price_increment\": quotation_to_decimal(\n",
    "                                    item.min_price_increment\n",
    "                                ),\n",
    "                                \"scale\": 9 - len(str(item.min_price_increment.nano)) + 1,\n",
    "                                \"lot\": item.lot,\n",
    "                                \"trading_status\": str(\n",
    "                                    SecurityTradingStatus(item.trading_status).name\n",
    "                                ),\n",
    "                                \"api_trade_available_flag\": item.api_trade_available_flag,\n",
    "                                \"currency\": item.currency,\n",
    "                                \"exchange\": item.exchange,\n",
    "                                \"buy_available_flag\": item.buy_available_flag,\n",
    "                                \"sell_available_flag\": item.sell_available_flag,\n",
    "                                \"short_enabled_flag\": item.short_enabled_flag,\n",
    "                                \"klong\": quotation_to_decimal(item.klong),\n",
    "                                \"kshort\": quotation_to_decimal(item.kshort),\n",
    "                            }\n",
    "                        )\n",
    "        \n",
    "            tickers_df = pd.DataFrame(tickers)\n",
    "            dump_pkl(tickers_df, './data/data_figi.pkl')\n",
    "            \n",
    "            flag_data = True\n",
    "        except:\n",
    "            time.sleep(3)\n",
    "\n",
    "    if not flag_data:\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'Failed to load figi data' +'\\n')\n",
    "\n",
    "\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End loading figi data' +'\\n')\n",
    "\n",
    "def get_figi(ticker):\n",
    "\n",
    "        df_figi = load_pkl('./data/data_figi.pkl')\n",
    "\n",
    "        df_figi = df_figi[df_figi[\"ticker\"] == ticker]\n",
    "        if ticker == 'T': # таких два тикера T\n",
    "            df_figi = df_figi[df_figi['currency'] == 'rub']\n",
    "            \n",
    "        assert df_figi.shape[0] == 1, f\"{ticker}: error figi\"\n",
    "\n",
    "        figi = df_figi[\"figi\"].iloc[0]\n",
    "        return figi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e3131-09a5-415c-ae1d-4d25ef2b3ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0982e974-be2c-47ee-be68-9564e4afba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_candles(ticker=None, from_=None, to_=None, interval=None):\n",
    "    \n",
    "    figi = get_figi(ticker)\n",
    "    \n",
    "    with Client(TOKEN) as client:\n",
    "        \n",
    "        data = [elem for elem in client.get_all_candles(figi=figi, \n",
    "                                      from_=from_-timedelta(hours=3),\n",
    "                                      to=to_- timedelta(hours=3),\n",
    "                                     interval=interval\n",
    "                                    )\n",
    "               ]\n",
    "        \n",
    "    close = np.zeros(len(data))\n",
    "    volume = np.zeros(len(data))\n",
    "    times = np.zeros(len(data), dtype='O')\n",
    "    for i, elem in enumerate(data):\n",
    "        close[i] = quotation_to_decimal(elem.close)\n",
    "        volume[i] = elem.volume\n",
    "        times[i] = elem.time\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'time' : times,\n",
    "                      'close' : close,\n",
    "                      'volume' : volume,\n",
    "                    })\n",
    "    df['time'] += pd.Timedelta(hours=3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda60da4-66e9-4219-8d33-8501fa46f9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6b1cf-008a-4e87-b2bc-04601ff77e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af7102-696a-44dd-9e38-c19fb72eebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e21c47f3-75e4-405b-8bc2-ffa570d7a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "#load_figi()\n",
    "\n",
    "# ticker = 'SFIN'\n",
    "# #2024-06-10 \n",
    "# data = get_all_candles(ticker=ticker,\n",
    "#                        from_=datetime.datetime(2024, 12, 29, tzinfo=datetime.timezone.utc),\n",
    "#                        to_=datetime.datetime(2025, 1, 29, tzinfo=datetime.timezone.utc),\n",
    "#                        interval=CandleInterval.CANDLE_INTERVAL_HOUR\n",
    "#                       )\n",
    "\n",
    "# data['ticker'] = ticker\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9284a6a8-260c-476b-aeac-7d0cb2662573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "#load_figi()\n",
    "\n",
    "\n",
    "# ticker = 'SFIN'\n",
    "# #2024-06-10 \n",
    "# data = get_all_candles(ticker=ticker,\n",
    "#                        from_=datetime.datetime(2025, 1, 20, tzinfo=datetime.timezone.utc),\n",
    "#                        to_=datetime.datetime(2025, 1, 29, tzinfo=datetime.timezone.utc),\n",
    "#                        interval=CandleInterval.CANDLE_INTERVAL_DAY\n",
    "#                       )\n",
    "\n",
    "# data['ticker'] = ticker\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3650dc5-409c-47f3-9966-a027d2c4dd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135fe20-116a-4ac9-8b58-cba86f47c24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1b4b2-f535-4ed9-906f-1e0545934073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bca12996-30bd-4d35-b359-4485d996a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_1day(stocks):\n",
    "    #loading figi\n",
    "    load_figi()\n",
    "\n",
    "    \n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start loading daily data' +'\\n')    \n",
    "    \n",
    "    #make dir\n",
    "    time_name = str(pd.Timestamp.now().date()) \n",
    "    if not os.path.exists(f'./data/raw/1day/{time_name}'):\n",
    "        os.mkdir(f'./data/raw/1day/{time_name}')\n",
    "\n",
    "    #loading data\n",
    "    end = pd.Timestamp.now()+pd.Timedelta(days=1) #+1day, чтобы выгрузить все, что есть (+1 точка для проверки актуальности данных)\n",
    "    start = end - pd.Timedelta(days=240)\n",
    "    \n",
    "    for ticker in tqdm(stocks+['TMOS']): \n",
    "        \n",
    "        count_tries = 0\n",
    "        max_count_tries = 4\n",
    "        flag_data = False\n",
    "        while (not flag_data) and (count_tries < max_count_tries):\n",
    "            count_tries += 1\n",
    "            try:\n",
    "                #spec case of X5\n",
    "                if ticker in ['X5', 'TMOS']:\n",
    "                    data = get_all_candles(ticker=ticker,\n",
    "                                   from_=start - pd.Timedelta(days=9*30),\n",
    "                                   to_=end,\n",
    "                                   interval=CandleInterval.CANDLE_INTERVAL_DAY)\n",
    "                else:\n",
    "                    data = get_all_candles(ticker=ticker,\n",
    "                                   from_=start,\n",
    "                                   to_=end,\n",
    "                                   interval=CandleInterval.CANDLE_INTERVAL_DAY)\n",
    "\n",
    "                flag_data = True\n",
    "            except:\n",
    "                time.sleep(3)    \n",
    "        #saving\n",
    "        if flag_data:\n",
    "            data = data[['time', 'close', 'volume']]\n",
    "            data['ticker'] = ticker\n",
    "            dump_pkl(data, f'./data/raw/1day/{time_name}/{ticker}.pkl')\n",
    "        else:\n",
    "            write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'{ticker}: failed to load daily data' +'\\n')\n",
    "            \n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End loading daily data' +'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data_1hour(stocks):\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start loading hourly data' +'\\n')\n",
    "\n",
    "    #make dir\n",
    "    time_name = str(pd.Timestamp.now()).split(':')[0].replace(' ', '_') #date+hour\n",
    "    if not os.path.exists(f'./data/raw/1hour/{time_name}'):\n",
    "        os.mkdir(f'./data/raw/1hour/{time_name}')\n",
    "    \n",
    "    #loading data\n",
    "    end = pd.Timestamp.now()+pd.Timedelta(days=1) #+1day, чтобы выгрузить все, что есть (+1 точка для проверки целостности данных)\n",
    "    start = end - pd.Timedelta(days=25)\n",
    "    for ticker in tqdm(stocks+['TMOS']): \n",
    "        \n",
    "        count_tries = 0\n",
    "        max_count_tries = 4\n",
    "        flag_data = False\n",
    "        while (not flag_data) and (count_tries < max_count_tries):\n",
    "            count_tries += 1\n",
    "            try:\n",
    "                data = get_all_candles(ticker=ticker,\n",
    "                               from_=start,\n",
    "                               to_=end,\n",
    "                               interval=CandleInterval.CANDLE_INTERVAL_HOUR)\n",
    "                flag_data = True\n",
    "            except:\n",
    "                time.sleep(3)    \n",
    "        #saving\n",
    "        if flag_data:\n",
    "            data = data[['time', 'close', 'volume']]\n",
    "            data['ticker'] = ticker\n",
    "            dump_pkl(data, f'./data/raw/1hour/{time_name}/{ticker}.pkl')\n",
    "        else:\n",
    "            write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'{ticker}: failed to load hourly data' +'\\n')\n",
    "\n",
    "    \n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End loading hourly data' +'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data_15min(stocks):\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start loading 15min data' +'\\n')\n",
    "\n",
    "    #make dir\n",
    "    time_name = f\"{str(pd.Timestamp.now()).split(':')[0].replace(' ', '_')}_{(pd.Timestamp.now().minute // 15) * 15}\"  #date+hour+15min\n",
    "    if not os.path.exists(f'./data/raw/15min/{time_name}'):\n",
    "        os.mkdir(f'./data/raw/15min/{time_name}')\n",
    "    \n",
    "    #loading data\n",
    "    end = pd.Timestamp.now()+pd.Timedelta(days=1) #+1day, чтобы выгрузить все, что есть (+1 точка для проверки целостности данных)\n",
    "    start = end - pd.Timedelta(days=8)\n",
    "    for ticker in tqdm(stocks+['TMOS']): \n",
    "        \n",
    "        count_tries = 0\n",
    "        max_count_tries = 4\n",
    "        flag_data = False\n",
    "        while (not flag_data) and (count_tries < max_count_tries):\n",
    "            count_tries += 1\n",
    "            try:\n",
    "                data = get_all_candles(ticker=ticker,\n",
    "                               from_=start,\n",
    "                               to_=end,\n",
    "                               interval=CandleInterval.CANDLE_INTERVAL_15_MIN)\n",
    "                flag_data = True\n",
    "            except:\n",
    "                time.sleep(3)    \n",
    "        #saving\n",
    "        if flag_data:\n",
    "            data = data[['time', 'close', 'volume']]\n",
    "            data['ticker'] = ticker\n",
    "            dump_pkl(data, f'./data/raw/15min/{time_name}/{ticker}.pkl')\n",
    "        else:\n",
    "            write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'{ticker}: failed to load 15min data' +'\\n')\n",
    "\n",
    "    \n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End loading 15min data' +'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1c773-f942-4f61-b3a7-9017538a09a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b605dd51-6f65-4328-a3a7-affc3e5864f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9293e88d-1071-4c0a-81ae-61004893bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_data_15min(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfaf9f54-c0d2-4441-90e7-848f11d0f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_data_1hour(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72f2782d-698e-4c52-87a1-765868a6fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_data_1day(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dc30535-b352-460b-88e6-824f369d6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_pkl('./data/raw/15min/2025-02-14_15_0/NLMK.pkl')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53458455-0b5e-45e0-96cc-ad1908e361cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_pkl('./data/raw/1hour/2025-02-14_15/NLMK.pkl')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f73ed123-5f5b-48f2-9351-3f40063715bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_pkl('./data/raw/1day/2025-02-14/X5.pkl')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724bc40-69a5-4002-9ec3-7cb405e49e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3de02e-0bdd-4a30-94e2-f4ecb68e0779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9b77f48-e902-49b2-b275-74e974a2e3bb",
   "metadata": {},
   "source": [
    "# 4. Preproc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc865009-b381-4e43-9042-423c12a8142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_business_day(date):\n",
    "    return bool(len(pd.bdate_range(date, date)))\n",
    "\n",
    "def time_cut_data_1day(data):\n",
    "    data['time'] = data['time'].apply(lambda x: x.tz_convert(None))\n",
    "    \n",
    "    mask_bd = np.array(data['time'].apply(lambda x: is_business_day(x)))\n",
    "    data = data.loc[mask_bd, :]\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def preproc_data_1day(stocks):\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start preproc daily data' +'\\n')\n",
    "\n",
    "    #make dir\n",
    "    time_name = str(pd.Timestamp.now().date()) \n",
    "    if not os.path.exists(f'./data/preproc/1day/{time_name}'):\n",
    "        os.mkdir(f'./data/preproc/1day/{time_name}')\n",
    "\n",
    "    #tmos\n",
    "    if not os.path.exists(f\"./data/raw/1day/{time_name}/TMOS.pkl\"):\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'TMOS: failed to preproc daily data' +'\\n')\n",
    "        return\n",
    "    data_tmos = load_pkl(f\"./data/raw/1day/{time_name}/TMOS.pkl\")\n",
    "\n",
    "    #preproc\n",
    "    for ticker in tqdm(stocks):\n",
    "        try:\n",
    "            data = load_pkl(f\"./data/raw/1day/{time_name}/{ticker}.pkl\")\n",
    "        \n",
    "            data_ext = data.merge(data_tmos[['time', 'close']].copy().rename(columns={\"close\" : \"tmos_close\"}), how='left', on='time')\n",
    "            assert data.shape[0] == data_ext.shape[0], 'Error join tmos'\n",
    "            \n",
    "            data = time_cut_data_1day(data_ext)\n",
    "            assert data['tmos_close'].isnull().sum() == 0, 'Error tmos_close nulls' #праздники, в них индекс не торгуется (мб в послед время рассчитывется?)\n",
    "            #data['tmos_close'] = data['tmos_close'].ffill()\n",
    "\n",
    "            #Actual time date (не самая идеальная реализация, так как чекает наличие следующей точки после прогнозной, а не прогнозную)\n",
    "            last_time = data['time'].iloc[-1]\n",
    "            assert last_time.date() == pd.Timestamp.now().date(), 'Error time_data' \n",
    "            \n",
    "            #Обрежем последнюю точку \n",
    "            mask = data['time'] < pd.Timestamp.now().floor('d')\n",
    "            data = data[mask]\n",
    "            \n",
    "            data = data.iloc[-(COUNT_POINTS+1):]\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            #len\n",
    "            assert data.shape[0] == (COUNT_POINTS+1), 'Error len'\n",
    "            \n",
    "            #Заполняемость\n",
    "            diff_time_day = data['time'].diff() / pd.Timedelta(hours=24)\n",
    "            assert (diff_time_day.iloc[1:] % 1 == 0).all(), 'Error filling 1'\n",
    "            assert (diff_time_day.iloc[1:] == 2).sum() < 5, 'Error filling 2'\n",
    "            # assert (diff_time_day.iloc[1:] < 10).all(), 'Error 5' #переезд акций ломает тест\n",
    "\n",
    "            #duplicates in time\n",
    "            assert (diff_time_day != 0).all(), 'Error time duplicates'\n",
    "\n",
    "            #sort in time\n",
    "            assert (diff_time_day.iloc[1:] > 0).all(), 'Error time sort'\n",
    "\n",
    "            #Nulls\n",
    "            assert not data.isnull().any().any(), 'Error nulls'  \n",
    "            \n",
    "            dump_pkl(data, f'./data/preproc/1day/{time_name}/{ticker}.pkl')\n",
    "        except Exception as exception:\n",
    "            write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'{ticker}: failed to preproc daily data ({exception})' +'\\n')\n",
    "\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End preproc daily data' +'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def time_cut_data_1hour_15min(data):\n",
    "    data['time'] = data['time'].apply(lambda x: x.tz_convert(None))\n",
    "    \n",
    "    mask_bd = np.array(data['time'].apply(lambda x: is_business_day(x)))\n",
    "    data = data.loc[mask_bd, :]\n",
    "\n",
    "    mask_volume = np.array(datetime.time(10, 0) <= pd.to_datetime(data['time'], format='%H:%M').dt.time)\n",
    "    data = data.loc[mask_volume, :]\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def preproc_data_1hour(stocks):\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start preproc hourly data' +'\\n')\n",
    "\n",
    "    #make dir\n",
    "    time_name = str(pd.Timestamp.now()).split(':')[0].replace(' ', '_') #date+hour\n",
    "    if not os.path.exists(f'./data/preproc/1hour/{time_name}'):\n",
    "        os.mkdir(f'./data/preproc/1hour/{time_name}')\n",
    "\n",
    "    #tmos\n",
    "    if not os.path.exists(f\"./data/raw/1hour/{time_name}/TMOS.pkl\"):\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'TMOS: failed to preproc hourly data' +'\\n')\n",
    "        return\n",
    "    data_tmos = load_pkl(f\"./data/raw/1hour/{time_name}/TMOS.pkl\")\n",
    "    #костыль\n",
    "    if data_tmos['time'].iloc[-1] == data_tmos['time'].iloc[-2]:\n",
    "        data_tmos = data_tmos.iloc[:-1].reset_index(drop=True)\n",
    "\n",
    "    #preproc\n",
    "    for ticker in tqdm(stocks):\n",
    "        try:\n",
    "            data = load_pkl(f\"./data/raw/1hour/{time_name}/{ticker}.pkl\")\n",
    "        \n",
    "            data_ext = data.merge(data_tmos[['time', 'close']].copy().rename(columns={\"close\" : \"tmos_close\"}), how='left', on='time')\n",
    "            assert data.shape[0] == data_ext.shape[0], 'Error join tmos'\n",
    "        \n",
    "            data = time_cut_data_1hour_15min(data_ext)\n",
    "            assert data['tmos_close'].isnull().sum() <= 5, 'Error tmos_close nulls' #Мосбиржа стоит (остаовка торгов), а акции на СПБ бирже торгуются (LSRG, SPBE, ...)\n",
    "            #костыль, но в рамках разумного, взял 5 - как кол-во часов в вечерней сессии\n",
    "            data['tmos_close'] = data['tmos_close'].ffill()\n",
    "\n",
    "            #Actual time date (не самая идеальная реализация, так как чекает наличие следующую точку после прогнозной, а не прогнозную)\n",
    "            last_time = data['time'].iloc[-1]\n",
    "            #округление времени до 1часа\n",
    "            cur_round_time = pd.Timestamp.now().floor('h')\n",
    "            assert (last_time == cur_round_time) or (last_time == (cur_round_time - pd.Timedelta(hours=1))), 'Error time_data' #послабление для малоликвидных акций\n",
    "            \n",
    "            \n",
    "            #Обрежем последнюю точку (или две, они бывают задваиваются - особенность онлайн данных)\n",
    "            mask = data['time'] < cur_round_time\n",
    "            data = data[mask]\n",
    "            \n",
    "            data = data.iloc[-(COUNT_POINTS+1):]\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            #len\n",
    "            assert data.shape[0] == (COUNT_POINTS+1), 'Error len'\n",
    "            \n",
    "            #Заполняемость\n",
    "            diff_time_hour = data['time'].diff() / pd.Timedelta(hours=1)\n",
    "            assert (diff_time_hour.iloc[1:] % 1 == 0).all(), 'Error filling 1'\n",
    "            assert (diff_time_hour.iloc[1:] == 2).sum() < 10, 'Error filling 2'\n",
    "            assert (diff_time_hour.iloc[1:] < 24*4).all(), 'Error filling 3' #допускает до 4 дней выходных\n",
    "\n",
    "            #duplicates in time\n",
    "            assert (diff_time_hour != 0).all(), 'Error time duplicates'\n",
    "\n",
    "            #sort in time\n",
    "            assert (diff_time_hour.iloc[1:] > 0).all(), 'Error time sort'\n",
    "            \n",
    "            #Nulls\n",
    "            assert not data.isnull().any().any(), 'Error nulls'\n",
    "    \n",
    "        \n",
    "            dump_pkl(data, f'./data/preproc/1hour/{time_name}/{ticker}.pkl')\n",
    "        except Exception as exception:\n",
    "            write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'{ticker}: failed to preproc hourly data ({exception})' +'\\n')\n",
    "\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End preproc hourly data' +'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preproc_data_15min(stocks):\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start preproc 15min data' +'\\n')\n",
    "\n",
    "    #make dir\n",
    "    time_name = f\"{str(pd.Timestamp.now()).split(':')[0].replace(' ', '_')}_{(pd.Timestamp.now().minute // 15) * 15}\"  #date+hour+15min\n",
    "    if not os.path.exists(f'./data/preproc/15min/{time_name}'):\n",
    "        os.mkdir(f'./data/preproc/15min/{time_name}')\n",
    "\n",
    "    #tmos\n",
    "    if not os.path.exists(f\"./data/raw/15min/{time_name}/TMOS.pkl\"):\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'TMOS: failed to preproc 15min data' +'\\n')\n",
    "        return\n",
    "    data_tmos = load_pkl(f\"./data/raw/15min/{time_name}/TMOS.pkl\")\n",
    "    #костыль\n",
    "    if data_tmos['time'].iloc[-1] == data_tmos['time'].iloc[-2]:\n",
    "        data_tmos = data_tmos.iloc[:-1].reset_index(drop=True)\n",
    "\n",
    "    #preproc\n",
    "    for ticker in tqdm(stocks):\n",
    "        try:\n",
    "            data = load_pkl(f\"./data/raw/15min/{time_name}/{ticker}.pkl\")\n",
    "        \n",
    "            data_ext = data.merge(data_tmos[['time', 'close']].copy().rename(columns={\"close\" : \"tmos_close\"}), how='left', on='time')\n",
    "            assert data.shape[0] == data_ext.shape[0], 'Error join tmos'\n",
    "        \n",
    "            data = time_cut_data_1hour_15min(data_ext)\n",
    "            assert data['tmos_close'].isnull().sum()  < 20, 'Error tmos_close nulls' #Мосбиржа стоит (остановка торгов), а акции на СПБ бирже торгуются (LSRG, SPBE, ...)\n",
    "            #костыль, но в рамках разумного\n",
    "            data['tmos_close'] = data['tmos_close'].ffill()\n",
    "\n",
    "            #Actual time date (не самая идеальная реализация, так как чекает наличие следующую точку после прогнозной, а не прогнозную)\n",
    "            last_time = data['time'].iloc[-1]\n",
    "            #округление времени до 15мин\n",
    "            cur_round_time = pd.Timestamp(f\"{str(pd.Timestamp.now()).split(':')[0]}:{(pd.Timestamp.now().minute // 15) * 15}\")\n",
    "            assert (last_time == cur_round_time) or (last_time == (cur_round_time -  pd.Timedelta(minutes=15))), 'Error time_data'#послабление для малоликвидных акций\n",
    "            \n",
    "            #Обрежем последнюю точку (или две, они бывают задваиваются - особенность онлайн данных)\n",
    "            mask = data['time'] < cur_round_time\n",
    "            data = data[mask]\n",
    "            \n",
    "            data = data.iloc[-(COUNT_POINTS+1):]\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            #len\n",
    "            assert data.shape[0] == (COUNT_POINTS+1), 'Error len'\n",
    "            \n",
    "            #Заполняемость\n",
    "            diff_time_15min = data['time'].diff() / pd.Timedelta(minutes=15)\n",
    "            assert (diff_time_15min.iloc[1:] % 1 == 0).all(), 'Error filling 1'\n",
    "            assert (diff_time_15min.iloc[1:] == 2).sum() < 10, 'Error filling 2'\n",
    "            assert (diff_time_15min.iloc[1:] < 5*24*4).all(), 'Error filling 3'\n",
    "\n",
    "            #duplicates in time\n",
    "            assert (diff_time_15min != 0).all(), 'Error time duplicates'\n",
    "\n",
    "            #sort in time\n",
    "            assert (diff_time_15min.iloc[1:] > 0).all(), 'Error time sort'\n",
    "            \n",
    "            #Nulls\n",
    "            assert not data.isnull().any().any(), 'Error nulls'\n",
    "    \n",
    "        \n",
    "            dump_pkl(data, f'./data/preproc/15min/{time_name}/{ticker}.pkl')\n",
    "        except Exception as exception:\n",
    "            write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'{ticker}: failed to preproc 15min data ({exception})' +'\\n')\n",
    "\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End preproc 15min data' +'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be4af3-b0a4-496a-9594-531af2c9a87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70f4a3c3-fe98-4784-828b-f60f22bcba15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preproc_data_1day(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f3318f1-f600-4ddd-956a-5fb77f80954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc_data_1hour(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ea02b25-87f4-4271-8556-8f93b566c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc_data_15min(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e043d1-63bb-4d1d-bb49-5048ee12a94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21f7c648-2ac1-45c8-96bc-d5214063d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_pkl('./data/preproc/1day/2025-02-14/YDEX.pkl')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "196d5ff7-df23-4b35-ad99-0074b8e5f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_pkl('./data/preproc/1hour/2025-02-14_15/YDEX.pkl')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8da4e770-b9e7-4b54-b1c1-47ed22afc3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_pkl('./data/preproc/15min/2025-02-14_15_45//YDEX.pkl')\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99289433-2efc-4b9b-9117-d404afcb7365",
   "metadata": {},
   "source": [
    "# 5. Prepare data to lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51b9f6bc-3955-4efd-81e0-6c81b27f7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def calculate_exp_ma(data, window):\n",
    "    return data.ewm(span=window, min_periods=window).mean().squeeze().values\n",
    "\n",
    "\n",
    "def calculate_bollinger_bands(data, window):\n",
    "    \"\"\"Calculate Bollinger Bands\"\"\"\n",
    "    rolling_mean = data.rolling(window=window, min_periods=1).mean().values\n",
    "    rolling_std = data.rolling(window=window, min_periods=1).std().values\n",
    "\n",
    "    norm_rolling_std = rolling_std/rolling_mean\n",
    "\n",
    "    num_of_std = 2\n",
    "    lower_band_2std = rolling_mean - (rolling_std * num_of_std)\n",
    "    upper_band_2std = rolling_mean + (rolling_std * num_of_std)\n",
    "    \n",
    "    num_of_std = 3\n",
    "    lower_band_3std = rolling_mean - (rolling_std * num_of_std)\n",
    "    upper_band_3std = rolling_mean + (rolling_std * num_of_std)\n",
    "    \n",
    "    \n",
    "    return rolling_mean, rolling_std, norm_rolling_std, lower_band_2std, upper_band_2std, lower_band_3std, upper_band_3std\n",
    "\n",
    "def calculate_rsi(data, window):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    delta = data.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    mask = avg_loss == 0\n",
    "    rsi[mask] = 100\n",
    "    \n",
    "    return rsi.values\n",
    "\n",
    "def calculate_roc(data, periods):\n",
    "    \"\"\"Calculate Rate of Change.\"\"\"\n",
    "    roc = ((data - data.shift(periods)) / data.shift(periods))\n",
    "    return roc.values\n",
    "\n",
    "\n",
    "\n",
    "def calc_stats(data, window=None, feat_name=None):\n",
    "    #mean, std\n",
    "    rolling_mean, rolling_std, norm_rolling_std,\\\n",
    "    lower_band_2std, upper_band_2std, lower_band_3std, upper_band_3std = calculate_bollinger_bands(data, window)\n",
    "\n",
    "    #mean_abs_pct\n",
    "    mean_abs_pct = data.pct_change(periods=1).rolling(window=window, min_periods=1).apply(lambda x: x.abs().mean())\n",
    "        \n",
    "    #alpha\n",
    "    alpha = data.rolling(window=window, min_periods=2).apply(lambda x: LinearRegression().fit(x.values.reshape(-1, 1), np.arange(x.shape[0])).coef_[0])\n",
    "\n",
    "    #min, max\n",
    "    rolling_min = data.rolling(window=window, min_periods=1).min().values\n",
    "    rolling_max = data.rolling(window=window, min_periods=1).max().values\n",
    "    \n",
    "    #rsi\n",
    "    rsi = calculate_rsi(data, window)\n",
    "    \n",
    "    #roc\n",
    "    roc = calculate_roc(data, window)\n",
    "    diff = data.diff(window).values\n",
    "\n",
    "    #exp_ma\n",
    "    exp_ma = calculate_exp_ma(data, window)\n",
    "    \n",
    "    df_features = pd.DataFrame({f'{feat_name}_ma' : rolling_mean,\n",
    "                        f'{feat_name}_std' : rolling_std,\n",
    "                        f'{feat_name}_norm_std' : norm_rolling_std,\n",
    "                        f'{feat_name}_ma_low_2std' : lower_band_2std,\n",
    "                        f'{feat_name}_ma_up_2std' : upper_band_2std,\n",
    "                        f'{feat_name}_ma_low_3std' : lower_band_3std,\n",
    "                        f'{feat_name}_ma_up_3std' : upper_band_3std, \n",
    "\n",
    "                        f'{feat_name}_mean_abs_pct' : mean_abs_pct,\n",
    "                            \n",
    "                        f'{feat_name}_alpha' : alpha,\n",
    "                            \n",
    "                        f'{feat_name}_min' : rolling_min,\n",
    "                        f'{feat_name}_max' : rolling_max,\n",
    "                        f'{feat_name}_rsi' : rsi,\n",
    "                        f'{feat_name}_roc' : roc,\n",
    "                        f'{feat_name}_diff' : diff,\n",
    "                        f'{feat_name}_expma' : exp_ma,\n",
    "                        })\n",
    "    return df_features\n",
    "\n",
    "\n",
    "def calc_stats_diff_1(data, feat_name=None):\n",
    "    return pd.DataFrame({f'{feat_name}_roc' : data.pct_change(periods=1).values,\n",
    "                        f'{feat_name}_diff' : data.diff(1).values,\n",
    "                        })\n",
    "\n",
    "def calc_levels(data, window=None, levels=None, feat_name=None):\n",
    "    \n",
    "    #уровни\n",
    "    data_levels = []\n",
    "    column_names = []\n",
    "    for i in range(1, len(levels)):\n",
    "        level_low = levels[i-1]\n",
    "        level_high = levels[i]\n",
    "        data_levels += [data.rolling(window=window, min_periods=1).apply(lambda x: (((1+level_low)*x.values[-1] < x.values) & (x.values <= (1+level_high)*x.values[-1])).sum()).values]\n",
    "        data_levels += [data.rolling(window=window, min_periods=1).apply(lambda x: (((1-level_high)*x.values[-1] <= x.values) & (x.values < (1-level_low)*x.values[-1])).sum()).values]\n",
    "\n",
    "        column_names += [f\"{feat_name}_lvl_{1+level_low}-{1+level_high}\"]\n",
    "        column_names += [f\"{feat_name}_lvl_-{1-level_high}-{1-level_low}\"]\n",
    "    df_levels = pd.DataFrame({column_names[i]:data_levels[i] for i in range(len(column_names))})\n",
    "    return df_levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbe3c971-9c63-453c-b3d8-9c2fd34feb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(df_ticker, postfix=None):\n",
    "    dfs = [df_ticker]\n",
    "    \n",
    "    levels =      [0, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.07]\n",
    "    levels_tmos = [0, 0.005, 0.01, 0.015, 0.02, 0.03, 0.04, 0.05]\n",
    "\n",
    "\n",
    "    #w1\n",
    "    df_close = calc_stats_diff_1(df_ticker[f'close{postfix}'], feat_name=f'close{postfix}_w1')\n",
    "    df_volume = calc_stats_diff_1(df_ticker[f'volume{postfix}'], feat_name=f'volume{postfix}_w1')\n",
    "    df_tmos_close = calc_stats_diff_1(df_ticker[f'tmos_close{postfix}'], feat_name=f'tmos_close{postfix}_w1')\n",
    "    assert df_ticker.shape[0] == df_close.shape[0] == df_volume.shape[0] == df_tmos_close.shape[0], 'Error w1'\n",
    "    dfs += [df_close.copy(), df_volume.copy(), df_tmos_close.copy()]\n",
    "\n",
    "    #w5\n",
    "    window = 5\n",
    "    df_close = calc_stats(df_ticker[f'close{postfix}'], window=window, feat_name=f'close{postfix}_w{window}')\n",
    "    df_volume = calc_stats(df_ticker[f'volume{postfix}'], window=window, feat_name=f'volume{postfix}_w{window}')\n",
    "    df_tmos_close = calc_stats(df_ticker[f'tmos_close{postfix}'], window=window, feat_name=f'tmos_close{postfix}_w{window}')\n",
    "    assert df_ticker.shape[0] == df_close.shape[0] == df_volume.shape[0] == df_tmos_close.shape[0], f'Error w{window}'\n",
    "    dfs += [df_close.copy(), df_volume.copy(), df_tmos_close.copy()]\n",
    "    \n",
    "    #w10\n",
    "    window = 10\n",
    "    df_close = calc_stats(df_ticker[f'close{postfix}'], window=window, feat_name=f'close{postfix}_w{window}')\n",
    "    df_volume = calc_stats(df_ticker[f'volume{postfix}'], window=window, feat_name=f'volume{postfix}_w{window}')\n",
    "    df_tmos_close = calc_stats(df_ticker[f'tmos_close{postfix}'], window=window, feat_name=f'tmos_close{postfix}_w{window}')\n",
    "    assert df_ticker.shape[0] == df_close.shape[0] == df_volume.shape[0] == df_tmos_close.shape[0], f'Error w{window}'\n",
    "    dfs += [df_close.copy(), df_volume.copy(), df_tmos_close.copy()]\n",
    "    \n",
    "    #w20\n",
    "    window = 20\n",
    "    df_close = calc_stats(df_ticker[f'close{postfix}'], window=window, feat_name=f'close{postfix}_w{window}')\n",
    "    df_volume = calc_stats(df_ticker[f'volume{postfix}'], window=window, feat_name=f'volume{postfix}_w{window}')\n",
    "    df_tmos_close = calc_stats(df_ticker[f'tmos_close{postfix}'], window=window, feat_name=f'tmos_close{postfix}_w{window}')\n",
    "    assert df_ticker.shape[0] == df_close.shape[0] == df_volume.shape[0] == df_tmos_close.shape[0], f'Error w{window}'\n",
    "    dfs += [df_close.copy(), df_volume.copy(), df_tmos_close.copy()]\n",
    "    \n",
    "    #w30\n",
    "    window = 30\n",
    "    df_close = calc_stats(df_ticker[f'close{postfix}'], window=window, feat_name=f'close{postfix}_w{window}')\n",
    "    #df_volume = calc_stats(df_ticker[f'volume{postfix}'], window=window, feat_name=f'volume{postfix}_w{window}')\n",
    "    df_tmos_close = calc_stats(df_ticker[f'tmos_close{postfix}'], window=window, feat_name=f'tmos_close{postfix}_w{window}')\n",
    "    df_close_levels = calc_levels(df_ticker[f'close{postfix}'], window=window, levels=levels, feat_name=f'close{postfix}_w{window}')\n",
    "    df_tmos_close_levels = calc_levels(df_ticker[f'tmos_close{postfix}'], window=window, levels=levels_tmos, feat_name=f'tmos_close{postfix}_w{window}')\n",
    "    assert df_ticker.shape[0] == df_close.shape[0] == df_tmos_close.shape[0] == df_close_levels.shape[0] == df_tmos_close_levels.shape[0], f'Error w{window}'\n",
    "    dfs += [df_close.copy(), df_tmos_close.copy(), df_close_levels.copy(), df_tmos_close_levels.copy()]\n",
    "    \n",
    "    #w60\n",
    "    window = 60\n",
    "    df_close = calc_stats(df_ticker[f'close{postfix}'], window=window, feat_name=f'close{postfix}_w{window}')\n",
    "    #df_volume = calc_stats(df_ticker[f'volume{postfix}'], window=window, feat_name=f'volume{postfix}_w{window}')\n",
    "    df_tmos_close = calc_stats(df_ticker[f'tmos_close{postfix}'], window=window, feat_name=f'tmos_close{postfix}_w{window}')\n",
    "    assert df_ticker.shape[0] == df_close.shape[0] == df_tmos_close.shape[0], f'Error w{window}'\n",
    "    dfs += [df_close.copy(), df_tmos_close.copy()]\n",
    "    \n",
    "    #w120\n",
    "    window = 120\n",
    "    df_close = calc_stats(df_ticker[f'close{postfix}'], window=window, feat_name=f'close{postfix}_w{window}')\n",
    "    #df_volume = calc_stats(df_ticker[f'volume{postfix}'], window=window, feat_name=f'volume{postfix}_w{window}')\n",
    "    df_tmos_close = calc_stats(df_ticker[f'tmos_close{postfix}'], window=window, feat_name=f'tmos_close{postfix}_w{window}')\n",
    "    df_close_levels = calc_levels(df_ticker[f'close{postfix}'], window=window, levels=levels, feat_name=f'close{postfix}_w{window}')\n",
    "    df_tmos_close_levels = calc_levels(df_ticker[f'tmos_close{postfix}'], window=window, levels=levels_tmos, feat_name=f'tmos_close{postfix}_w{window}')\n",
    "    assert df_ticker.shape[0] == df_close.shape[0] == df_tmos_close.shape[0] == df_close_levels.shape[0] == df_tmos_close_levels.shape[0], f'Error w{window}'\n",
    "    dfs += [df_close.copy(), df_tmos_close.copy(), df_close_levels.copy(), df_tmos_close_levels.copy()]\n",
    "\n",
    "   \n",
    "    df = pd.concat(dfs, axis=1)\n",
    "    assert (df_ticker.shape[0] == df.shape[0]) and (df.shape[1] == sum([elem.shape[1] for elem in dfs])), 'Error concat'\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66d9b5-f6a8-4894-a1b4-24beab1cb802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba5e7c-b332-4cf4-acfe-77cd869b83a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6a2dd-ee24-49c4-afc4-105802ee7169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e00ac5-e53f-4041-99fb-188d276d6eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1de949ab-0cd6-4b9e-b640-69e6a2d6be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniq_pairs(cols):\n",
    "    pairs = []\n",
    "    for i in range(len(cols)-1):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            pairs += [(cols[i], cols[j])]\n",
    "    return pairs\n",
    "\n",
    "def calc_relative_features(df, groups):\n",
    "    for group in groups:\n",
    "        if type(group) == list:\n",
    "            pairs = uniq_pairs(group)\n",
    "            for pair in pairs:\n",
    "                new_col = f'{pair[0]}/{pair[1]}'\n",
    "                df[new_col] = df[pair[0]] / (df[pair[1]] + np.finfo(np.float32).eps)\n",
    "\n",
    "        if type(group) == dict:\n",
    "            pair1 = list(group.keys())[0]\n",
    "            for pair0 in group[pair1]:\n",
    "                new_col = f'{pair0}/{pair1}'\n",
    "                df[new_col] = df[pair0] / (df[pair1] + np.finfo(np.float32).eps)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f318efb-141e-4ca4-ac56-eb007bbe338d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1207040c-6f30-4954-a401-f82bd2696517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab4bc073-8c74-4e58-b182-cce9eb9ca666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_lgbm(stocks, features):\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start prepare data to lgbm' +'\\n')\n",
    "\n",
    "    name_day = str(pd.Timestamp.now().date()) #date\n",
    "    name_hour = str(pd.Timestamp.now()).split(':')[0].replace(' ', '_') #date+hour\n",
    "    name_15min = f\"{str(pd.Timestamp.now()).split(':')[0].replace(' ', '_')}_{(pd.Timestamp.now().minute // 15) * 15}\"  #date+hour+15min\n",
    "\n",
    "    #prepare\n",
    "    data_lgbm = []\n",
    "\n",
    "    #тут можно ускорить в 120 раз, если считать только последнюю точку в каждом df (можно еще считать только нужные фичи - но это перебор, плохо масштабируется)\n",
    "    for ticker in tqdm(stocks):\n",
    "        try:\n",
    "            df_1day = load_pkl(f\"./data/preproc/1day/{name_day}/{ticker}.pkl\")\n",
    "            df_1hour = load_pkl(f\"./data/preproc/1hour/{name_hour}/{ticker}.pkl\")\n",
    "            df_15min = load_pkl(f\"./data/preproc/15min/{name_15min}/{ticker}.pkl\")\n",
    "\n",
    "            df_1day = df_1day.rename(columns={col : col+'_1day' for col in df_1day.columns})\n",
    "            df_1hour = df_1hour.rename(columns={col : col+'_1hour' for col in df_1hour.columns})\n",
    "\n",
    "            # вот тут\n",
    "            df_1day = calculate_features(df_1day, postfix='_1day').iloc[-1:].reset_index(drop=True)\n",
    "            df_1hour = calculate_features(df_1hour, postfix='_1hour').iloc[-1:].reset_index(drop=True)\n",
    "            df_15min = calculate_features(df_15min, postfix='').iloc[-1:].reset_index(drop=True)\n",
    "\n",
    "            #time features\n",
    "            df_15min['hour'] = df_15min['time'].dt.hour\n",
    "            df_15min['day'] = df_15min['time'].dt.day\n",
    "            df_15min['weekday'] = np.minimum(df_15min['time'].dt.dayofweek, 4) / 4\n",
    "\n",
    "            assert not df_1day.isnull().any().any(), 'Error nulls df_1day'\n",
    "            assert not df_1hour.isnull().any().any(), 'Error nulls df_1hour'\n",
    "            assert not df_15min.isnull().any().any(), 'Error nulls df_15min'\n",
    "            \n",
    "            data = pd.concat([df_15min, df_1hour, df_1day], axis=1)\n",
    "        \n",
    "            data_lgbm += [data.copy()]\n",
    "            \n",
    "        except Exception as exception:\n",
    "            write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'{ticker}: failed to prepare data to lgbm ({exception})' +'\\n')\n",
    "\n",
    "    try:\n",
    "        data_lgbm = pd.concat(data_lgbm)\n",
    "        data_lgbm = calc_relative_features(data_lgbm, GROUPS_1DAY+GROUPS_1HOUR+GROUPS_15MIN)\n",
    "        \n",
    "        assert not data_lgbm.isnull().any().any(), 'Error nulls data_lgbm'  \n",
    "        \n",
    "        data_lgbm = data_lgbm[['time', 'ticker', 'close', 'time_1hour', 'time_1day'] + features].reset_index(drop=True).copy()\n",
    "        dump_pkl(data_lgbm, f\"./data/lgbm/data_lgbm_{name_15min}.pkl\")\n",
    "\n",
    "    except Exception as exception:\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' '+ f'Failed to prepare data to lgbm ({exception})' +'\\n')\n",
    "\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End prepare data to lgbm' +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259cff2-304d-4762-97de-8f8d7a21b50d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc3413cc-568a-4007-b41e-61bc3c69b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data_lgbm(STOCKS, FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f22513-b882-47fe-bc66-2070d6a5b436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02414c22-f99f-49fa-a986-0ea4ac98dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = load_pkl('./data/lgbm/data_lgbm_2025-02-04_16.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd975ebe-ecd4-4958-a737-c851b03eb0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163a5db-1e83-4297-8938-281f0d2888d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36eac7e8-f4c1-41e8-8b47-73601e44b955",
   "metadata": {},
   "source": [
    "# 6. predict lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f48b9e36-6404-453b-b04f-92dd2c1af3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lgbm_model:\n",
    "    def __init__(self, name=None, strategy=None, threshold=None, path=None, features=None):\n",
    "        self.name = name\n",
    "        self.strategy = strategy\n",
    "        self.threshold = threshold\n",
    "        self.model = load_pkl(path)\n",
    "        self.features = features\n",
    "        \n",
    "    def predict(self, data_cp):\n",
    "        data = data_cp.copy()\n",
    "\n",
    "        data['model'] = self.name\n",
    "        \n",
    "        data['y_pred'] = self.model.predict(data[self.features])\n",
    "        data['y_pred_bin'] = (data['y_pred'].values >= self.threshold).astype(int)\n",
    "    \n",
    "        #можно для больших чисел без 6 знаков после запятой сделать\n",
    "        data['fix_lose'] = np.round(data['close'] * self.strategy[0], 6)\n",
    "        data['fix_win'] = np.round(data['close'] * self.strategy[1], 6)\n",
    "    \n",
    "        data = data[['time', 'ticker', 'close', 'model', 'y_pred', 'y_pred_bin', 'fix_lose', 'fix_win']]\n",
    "\n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51cd1416-878a-41a7-bef1-a4e1e56619f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL1 = lgbm_model(name='short_+0.5_-1.5_4hour', \n",
    "#                     strategy=(1.005, 0.985), \n",
    "#                     threshold=0.16409330252458745, #99\n",
    "#                     path='../bst_mdl/model_+0.5_-1.5_4hour/model_+0.5_-1.5_4hour.pkl', \n",
    "#                     features=MODEL_FEATURES1)\n",
    "\n",
    "MODEL2 = lgbm_model(name='short_+0.5_-1.5_4hour_v2', \n",
    "                    strategy=(1.005, 0.985), \n",
    "                    threshold=0.3095578175723138, #995\n",
    "                    path='../bst_mdl/model_+0.5_-1.5_4hour_v2/model_+0.5_-1.5_4hour.pkl', \n",
    "                    features=MODEL_FEATURES1)\n",
    "\n",
    "\n",
    "MODEL3 = lgbm_model(name='long_-0.5_+1.5_4hour', \n",
    "                    strategy=(0.995, 1.015), \n",
    "                    threshold=0.2591240486482249, #99\n",
    "                    path='../bst_mdl/model_-0.5_+1.5_4hour/model_-0.5_+1.5_4hour.pkl', \n",
    "                    features=MODEL_FEATURES2)\n",
    "\n",
    "\n",
    "\n",
    "MODELS = [#MODEL1,\n",
    "    MODEL2,\n",
    "    MODEL3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e1b82-84ce-4f46-b3b3-7012b96ba9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccff21e-6986-482e-a056-cb6ccd0e3c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81305c8f-4075-4d67-92e9-32759c5c70b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8283d53-db4b-4c69-a81a-45957adeac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lgbm(models):\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start lgbm predicting' +'\\n')\n",
    "\n",
    "    try:\n",
    "        name_15min = f\"{str(pd.Timestamp.now()).split(':')[0].replace(' ', '_')}_{(pd.Timestamp.now().minute // 15) * 15}\"  #date+hour+15min\n",
    "        data = load_pkl(f\"./data/lgbm/data_lgbm_{name_15min}.pkl\")\n",
    "\n",
    "        data_result = []\n",
    "        for model in models:\n",
    "            data_result += [model.predict(data)]\n",
    "            \n",
    "        data_result = pd.concat(data_result).reset_index(drop=True)\n",
    "        \n",
    "        dump_pkl(data_result, f\"./data/result/data_result_{name_15min}.pkl\")\n",
    "    except Exception as exception:\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' ' + f'Failed lgbm predicting ({exception})' +'\\n')\n",
    "\n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End lgbm predicting' +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b08533f-3e3e-43e9-82d2-ed8bb732e9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988c862-8cd4-4d33-afc5-667dbb71a9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81640d3f-df60-4599-a6df-40ca85fdc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_lgbm(MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db70a3dd-9809-4397-9558-f6df5460eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_pkl('./data/result/data_result_2025-02-14_16_15.pkl')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612f1c6-b88c-4d34-9167-122e2fe5432b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc5af7e0-8e00-42f7-9125-8bb21b5e59c5",
   "metadata": {},
   "source": [
    "# 7. Notification in telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8a86a89-7123-49c2-8b47-c84e62e7ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def escape_markdown(text):\n",
    "    \"\"\"функция для экранирования символов перед отправкой в маркдауне телеграма\"\"\"\n",
    "    pattern = r\"([_*\\[\\]()~|`])\"\n",
    "    return re.sub(pattern, r\"\\\\\\1\", text)\n",
    "\n",
    "def telegram_bot_sendtext(bot_message):\n",
    "    #bot_message = escape_markdown(bot_message)\n",
    "    send_text = 'https://api.telegram.org/bot' + BOT_TOKEN + '/sendMessage?chat_id=' + BOT_CHAT_ID + '&parse_mode=Markdown&text=' + bot_message\n",
    "    response = requests.get(send_text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "# test = telegram_bot_sendtext(\"Testing Telegram bot\")\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973cdc49-2023-4dec-9fb8-07e2dafb0f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40f36e09-2009-4209-a177-1b1b7ab62970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_pkl('./data/result/data_result_2025-02-04_16.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cceb79aa-d65c-46cf-9a81-d433c74beb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_notification_tg(stocks):    \n",
    "    #logs\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'Start sending msg' +'\\n')\n",
    "    \n",
    "    try:\n",
    "        name_15min = f\"{str(pd.Timestamp.now()).split(':')[0].replace(' ', '_')}_{(pd.Timestamp.now().minute // 15) * 15}\"  #date+hour+15min\n",
    "        data = load_pkl(f\"./data/result/data_result_{name_15min}.pkl\")\n",
    "\n",
    "        msg = f'*signals:* {data['y_pred_bin'].sum()}\\n'\n",
    "\n",
    "        for model in data['model'].unique():\n",
    "            msg += f\"\\n*{model}*\\n\\n\"\n",
    "            data_model = data[data['model'] == model]\n",
    "            for i, row in data_model.iterrows():\n",
    "                if row['y_pred_bin'] == 1:\n",
    "                    msg += f\"{row['ticker']}, close = {row['close']}\\n\"\n",
    "                    sign = '<' if row['fix_lose'] < row['fix_win'] else '>'\n",
    "                    msg += f\"{row['fix_lose']} {sign} {row['fix_win']}\\n\\n\"\n",
    "\n",
    "        msg += '\\n*no data:*\\n'\n",
    "        no_data_stocks = set(stocks).difference(set(data['ticker'].tolist()))\n",
    "        msg += \"\\n\".join(list(no_data_stocks))\n",
    "        \n",
    "        _ = telegram_bot_sendtext(msg)\n",
    "              \n",
    "    except Exception as exception:\n",
    "        write_logs(str(pd.Timestamp.now().round('s')) + ' ' + f'Failed sending msg ({exception})' +'\\n')\n",
    "\n",
    "    write_logs(str(pd.Timestamp.now().round('s')) + ' ' + 'End sending msg' +'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47cacd-0877-4223-b74c-6f223b23e356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57414084-3fb6-46d6-a226-ab35bcc70b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send_notification_tg(STOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057405ed-1a19-497a-9c74-bcfe534fb8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2eace-a352-4976-8108-c3532520eab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "066f7791-9f74-451d-88b9-21826228eec2",
   "metadata": {},
   "source": [
    "# x. Daemon Khibiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ebc500b-b39c-4c1f-9a97-97b2c14ec509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_business_day(date):\n",
    "    return bool(len(pd.bdate_range(date, date)))\n",
    "\n",
    "def flag_work_1day(date):\n",
    "    name_day = str(pd.Timestamp.now().date()) #date\n",
    "    if (date.hour >= 10) and (date.minute >= 0) and is_business_day(date) and (not os.path.exists(f'./data/raw/1day/{name_day}')):\n",
    "        return True\n",
    "    if (date.hour == 10) and (date.minute == 10) and is_business_day(date): #дополнительное скачивание на всякий случай, чтобы не терять целый день в худшем случае\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def flag_work_1hour(date):\n",
    "    name_hour = str(pd.Timestamp.now()).split(':')[0].replace(' ', '_') #date+hour\n",
    "    if (date.hour >= 10) and (date.minute >= 0) and is_business_day(date) and (not os.path.exists(f'./data/raw/1hour/{name_hour}')):\n",
    "        return True\n",
    "    if (date.hour >= 10) and (date.minute == 5) and is_business_day(date):\n",
    "        return True\n",
    "        \n",
    "    return False   \n",
    "\n",
    "def flag_work_15min(date):\n",
    "    name_15min = f\"{str(pd.Timestamp.now()).split(':')[0].replace(' ', '_')}_{(pd.Timestamp.now().minute // 15) * 15}\"  #date+hour+15min\n",
    "    cur_round_15min = (pd.Timestamp.now().minute // 15) * 15\n",
    "    if (date.hour >= 10) and (date.minute >= cur_round_15min) and is_business_day(date) and (not os.path.exists(f'./data/raw/15min/{name_15min}')):\n",
    "        return True\n",
    "    return False   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22eed5-d5ed-464e-9b47-3d7226771270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a0daa38-54bf-45b7-b0f6-8c619f61c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 15:30:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|███████████████████████████████████████████████████████████              | 55/68 [00:24<00:05,  2.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 118\u001b[0m, in \u001b[0;36mget_data_15min\u001b[0;34m(stocks)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     data \u001b[38;5;241m=\u001b[39m get_all_candles(ticker\u001b[38;5;241m=\u001b[39mticker,\n\u001b[1;32m    119\u001b[0m                    from_\u001b[38;5;241m=\u001b[39mstart,\n\u001b[1;32m    120\u001b[0m                    to_\u001b[38;5;241m=\u001b[39mend,\n\u001b[1;32m    121\u001b[0m                    interval\u001b[38;5;241m=\u001b[39mCandleInterval\u001b[38;5;241m.\u001b[39mCANDLE_INTERVAL_15_MIN)\n\u001b[1;32m    122\u001b[0m     flag_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m, in \u001b[0;36mget_all_candles\u001b[0;34m(ticker, from_, to_, interval)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Client(TOKEN) \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[0;32m----> 7\u001b[0m     data \u001b[38;5;241m=\u001b[39m [elem \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m client\u001b[38;5;241m.\u001b[39mget_all_candles(figi\u001b[38;5;241m=\u001b[39mfigi, \n\u001b[1;32m      8\u001b[0m                                   from_\u001b[38;5;241m=\u001b[39mfrom_\u001b[38;5;241m-\u001b[39mtimedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m      9\u001b[0m                                   to\u001b[38;5;241m=\u001b[39mto_\u001b[38;5;241m-\u001b[39m timedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     10\u001b[0m                                  interval\u001b[38;5;241m=\u001b[39minterval\n\u001b[1;32m     11\u001b[0m                                 )\n\u001b[1;32m     12\u001b[0m            ]\n\u001b[1;32m     14\u001b[0m close \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tinkoff/invest/services.py:272\u001b[0m, in \u001b[0;36mServices.get_all_candles\u001b[0;34m(self, from_, to, interval, figi, instrument_id, candle_source_type)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m current_from, current_to \u001b[38;5;129;01min\u001b[39;00m get_intervals(interval, from_, to):\n\u001b[0;32m--> 272\u001b[0m     candles_response: GetCandlesResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarket_data\u001b[38;5;241m.\u001b[39mget_candles(\n\u001b[1;32m    273\u001b[0m         figi\u001b[38;5;241m=\u001b[39mfigi,\n\u001b[1;32m    274\u001b[0m         interval\u001b[38;5;241m=\u001b[39minterval,\n\u001b[1;32m    275\u001b[0m         from_\u001b[38;5;241m=\u001b[39mcurrent_from,\n\u001b[1;32m    276\u001b[0m         to\u001b[38;5;241m=\u001b[39mcurrent_to,\n\u001b[1;32m    277\u001b[0m         instrument_id\u001b[38;5;241m=\u001b[39minstrument_id,\n\u001b[1;32m    278\u001b[0m         candle_source_type\u001b[38;5;241m=\u001b[39mcandle_source_type,\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m candle \u001b[38;5;129;01min\u001b[39;00m candles_response\u001b[38;5;241m.\u001b[39mcandles:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tinkoff/invest/_errors.py:23\u001b[0m, in \u001b[0;36mhandle_request_error.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RpcError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tinkoff/invest/services.py:924\u001b[0m, in \u001b[0;36mMarketDataService.get_candles\u001b[0;34m(self, figi, from_, to, interval, instrument_id, candle_source_type)\u001b[0m\n\u001b[1;32m    923\u001b[0m request\u001b[38;5;241m.\u001b[39minterval \u001b[38;5;241m=\u001b[39m interval\n\u001b[0;32m--> 924\u001b[0m response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub\u001b[38;5;241m.\u001b[39mGetCandles\u001b[38;5;241m.\u001b[39mwith_call(\n\u001b[1;32m    925\u001b[0m     request\u001b[38;5;241m=\u001b[39m_grpc_helpers\u001b[38;5;241m.\u001b[39mdataclass_to_protobuff(\n\u001b[1;32m    926\u001b[0m         request, marketdata_pb2\u001b[38;5;241m.\u001b[39mGetCandlesRequest()\n\u001b[1;32m    927\u001b[0m     ),\n\u001b[1;32m    928\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    929\u001b[0m )\n\u001b[1;32m    930\u001b[0m log_request(get_tracking_id_from_call(call), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetCandles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/grpc/_channel.py:1195\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_call\u001b[39m(\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1185\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1191\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[1;32m   1192\u001b[0m     (\n\u001b[1;32m   1193\u001b[0m         state,\n\u001b[1;32m   1194\u001b[0m         call,\n\u001b[0;32m-> 1195\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1196\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1197\u001b[0m     )\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/grpc/_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[0;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:62\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:58\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._interpret_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/tag.pyx.pxi:71\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._BatchOperationTag.event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/operation.pyx.pxi:138\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.ReceiveInitialMetadataOperation.un_c\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:69\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._metadata\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:70\u001b[0m, in \u001b[0;36mgenexpr\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:64\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._metadatum\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, key, value)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     preproc_data_1hour(STOCKS)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flag_work_15min(cur_time):\n\u001b[0;32m---> 15\u001b[0m     get_data_15min(STOCKS)\n\u001b[1;32m     16\u001b[0m     preproc_data_15min(STOCKS)    \n\u001b[1;32m     18\u001b[0m     prepare_data_lgbm(STOCKS, FEATURES)\n",
      "Cell \u001b[0;32mIn[20], line 124\u001b[0m, in \u001b[0;36mget_data_15min\u001b[0;34m(stocks)\u001b[0m\n\u001b[1;32m    122\u001b[0m         flag_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m--> 124\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)    \n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m#saving\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flag_data:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while 1 > 0:\n",
    "    clear_output()\n",
    "    cur_time = pd.Timestamp.now().floor('s')\n",
    "    print(cur_time)\n",
    "    \n",
    "    if flag_work_1day(cur_time):\n",
    "        get_data_1day(STOCKS)\n",
    "        preproc_data_1day(STOCKS)\n",
    "        \n",
    "    if flag_work_1hour(cur_time):   \n",
    "        get_data_1hour(STOCKS)\n",
    "        preproc_data_1hour(STOCKS)\n",
    "\n",
    "    if flag_work_15min(cur_time):\n",
    "        get_data_15min(STOCKS)\n",
    "        preproc_data_15min(STOCKS)    \n",
    "        \n",
    "        prepare_data_lgbm(STOCKS, FEATURES)\n",
    "        predict_lgbm(MODELS)\n",
    "        send_notification_tg(STOCKS)\n",
    "        \n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b2dfd-ad60-4a23-8f2d-d864fbab3c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d649d8f-c678-420c-8f4f-c3d0f1538a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_pkl('./data/result/data_result_2025-02-19_21_30.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965393a-fa37-4268-80bc-0f0518b6e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y_pred'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e77c3f-8a53-49ec-ba0e-141dfb82ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Часовая и 15 минутная дата в 10 часов (Error_time_data)\n",
    "\n",
    "посмотреть, что там за дата\n",
    "\n",
    "Может он и не должен отрабатывать в это время? хотя скорее там со временм не понятно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b589b-682a-41b7-9ef5-e5b83f27a8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b2484-8be3-40d1-b7d3-266c2d548d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
